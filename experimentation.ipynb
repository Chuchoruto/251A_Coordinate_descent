{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "df['target'] = wine.target\n",
    "df = df[df['target'] < 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols   \n",
      "0      14.23        1.71  2.43               15.6      127.0           2.80  \\\n",
      "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
      "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
      "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
      "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
      "..       ...         ...   ...                ...        ...            ...   \n",
      "125    12.07        2.16  2.17               21.0       85.0           2.60   \n",
      "126    12.43        1.53  2.29               21.5       86.0           2.74   \n",
      "127    11.79        2.13  2.78               28.5       92.0           2.13   \n",
      "128    12.37        1.63  2.30               24.5       88.0           2.22   \n",
      "129    12.04        4.30  2.38               22.0       80.0           2.10   \n",
      "\n",
      "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue   \n",
      "0          3.06                  0.28             2.29             5.64  1.04  \\\n",
      "1          2.76                  0.26             1.28             4.38  1.05   \n",
      "2          3.24                  0.30             2.81             5.68  1.03   \n",
      "3          3.49                  0.24             2.18             7.80  0.86   \n",
      "4          2.69                  0.39             1.82             4.32  1.04   \n",
      "..          ...                   ...              ...              ...   ...   \n",
      "125        2.65                  0.37             1.35             2.76  0.86   \n",
      "126        3.15                  0.39             1.77             3.94  0.69   \n",
      "127        2.24                  0.58             1.76             3.00  0.97   \n",
      "128        2.45                  0.40             1.90             2.12  0.89   \n",
      "129        1.75                  0.42             1.35             2.60  0.79   \n",
      "\n",
      "     od280/od315_of_diluted_wines  proline  target  \n",
      "0                            3.92   1065.0       0  \n",
      "1                            3.40   1050.0       0  \n",
      "2                            3.17   1185.0       0  \n",
      "3                            3.45   1480.0       0  \n",
      "4                            2.93    735.0       0  \n",
      "..                            ...      ...     ...  \n",
      "125                          3.28    378.0       1  \n",
      "126                          2.84    352.0       1  \n",
      "127                          2.44    466.0       1  \n",
      "128                          2.78    342.0       1  \n",
      "129                          2.57    580.0       1  \n",
      "\n",
      "[130 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9615384615384616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Prepare features and labels\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test) with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(penalty='l2', solver='saga')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.5158783355150558\n",
      "\n",
      "--- Outer Iteration 1/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.5159\n",
      "[Coordinate 1, Post-Update Loss = 0.5051\n",
      "\n",
      "--- Outer Iteration 2/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.5051\n",
      "[Coordinate 1, Post-Update Loss = 0.4948\n",
      "\n",
      "--- Outer Iteration 3/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4948\n",
      "[Coordinate 1, Post-Update Loss = 0.4850\n",
      "\n",
      "--- Outer Iteration 4/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4850\n",
      "[Coordinate 1, Post-Update Loss = 0.4757\n",
      "\n",
      "--- Outer Iteration 5/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4757\n",
      "[Coordinate 1, Post-Update Loss = 0.4667\n",
      "\n",
      "--- Outer Iteration 6/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4667\n",
      "[Coordinate 1, Post-Update Loss = 0.4582\n",
      "\n",
      "--- Outer Iteration 7/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4582\n",
      "[Coordinate 1, Post-Update Loss = 0.4501\n",
      "\n",
      "--- Outer Iteration 8/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4501\n",
      "[Coordinate 1, Post-Update Loss = 0.4424\n",
      "\n",
      "--- Outer Iteration 9/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4424\n",
      "[Coordinate 1, Post-Update Loss = 0.4349\n",
      "\n",
      "--- Outer Iteration 10/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4349\n",
      "[Coordinate 1, Post-Update Loss = 0.4278\n",
      "\n",
      "--- Outer Iteration 11/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.4278\n",
      "[Coordinate 13, Post-Update Loss = 0.4210\n",
      "\n",
      "--- Outer Iteration 12/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4210\n",
      "[Coordinate 1, Post-Update Loss = 0.4144\n",
      "\n",
      "--- Outer Iteration 13/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4144\n",
      "[Coordinate 1, Post-Update Loss = 0.4081\n",
      "\n",
      "--- Outer Iteration 14/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.4081\n",
      "[Coordinate 1, Post-Update Loss = 0.4021\n",
      "\n",
      "--- Outer Iteration 15/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.4021\n",
      "[Coordinate 13, Post-Update Loss = 0.3962\n",
      "\n",
      "--- Outer Iteration 16/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3962\n",
      "[Coordinate 1, Post-Update Loss = 0.3906\n",
      "\n",
      "--- Outer Iteration 17/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3906\n",
      "[Coordinate 1, Post-Update Loss = 0.3851\n",
      "\n",
      "--- Outer Iteration 18/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3851\n",
      "[Coordinate 13, Post-Update Loss = 0.3799\n",
      "\n",
      "--- Outer Iteration 19/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3799\n",
      "[Coordinate 1, Post-Update Loss = 0.3749\n",
      "\n",
      "--- Outer Iteration 20/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3749\n",
      "[Coordinate 1, Post-Update Loss = 0.3700\n",
      "\n",
      "--- Outer Iteration 21/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3700\n",
      "[Coordinate 13, Post-Update Loss = 0.3653\n",
      "\n",
      "--- Outer Iteration 22/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3653\n",
      "[Coordinate 1, Post-Update Loss = 0.3607\n",
      "\n",
      "--- Outer Iteration 23/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3607\n",
      "[Coordinate 1, Post-Update Loss = 0.3563\n",
      "\n",
      "--- Outer Iteration 24/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3563\n",
      "[Coordinate 1, Post-Update Loss = 0.3521\n",
      "\n",
      "--- Outer Iteration 25/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3521\n",
      "[Coordinate 13, Post-Update Loss = 0.3480\n",
      "\n",
      "--- Outer Iteration 26/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3480\n",
      "[Coordinate 1, Post-Update Loss = 0.3440\n",
      "\n",
      "--- Outer Iteration 27/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3440\n",
      "[Coordinate 1, Post-Update Loss = 0.3401\n",
      "\n",
      "--- Outer Iteration 28/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3401\n",
      "[Coordinate 13, Post-Update Loss = 0.3364\n",
      "\n",
      "--- Outer Iteration 29/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3364\n",
      "[Coordinate 1, Post-Update Loss = 0.3328\n",
      "\n",
      "--- Outer Iteration 30/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3328\n",
      "[Coordinate 1, Post-Update Loss = 0.3293\n",
      "\n",
      "--- Outer Iteration 31/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3293\n",
      "[Coordinate 13, Post-Update Loss = 0.3259\n",
      "\n",
      "--- Outer Iteration 32/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3259\n",
      "[Coordinate 1, Post-Update Loss = 0.3225\n",
      "\n",
      "--- Outer Iteration 33/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3225\n",
      "[Coordinate 1, Post-Update Loss = 0.3193\n",
      "\n",
      "--- Outer Iteration 34/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3193\n",
      "[Coordinate 1, Post-Update Loss = 0.3162\n",
      "\n",
      "--- Outer Iteration 35/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3162\n",
      "[Coordinate 13, Post-Update Loss = 0.3132\n",
      "\n",
      "--- Outer Iteration 36/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3132\n",
      "[Coordinate 1, Post-Update Loss = 0.3102\n",
      "\n",
      "--- Outer Iteration 37/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3102\n",
      "[Coordinate 1, Post-Update Loss = 0.3074\n",
      "\n",
      "--- Outer Iteration 38/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.3074\n",
      "[Coordinate 13, Post-Update Loss = 0.3046\n",
      "\n",
      "--- Outer Iteration 39/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3046\n",
      "[Coordinate 1, Post-Update Loss = 0.3019\n",
      "\n",
      "--- Outer Iteration 40/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.3019\n",
      "[Coordinate 1, Post-Update Loss = 0.2992\n",
      "\n",
      "--- Outer Iteration 41/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2992\n",
      "[Coordinate 13, Post-Update Loss = 0.2966\n",
      "\n",
      "--- Outer Iteration 42/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2966\n",
      "[Coordinate 1, Post-Update Loss = 0.2941\n",
      "\n",
      "--- Outer Iteration 43/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2941\n",
      "[Coordinate 1, Post-Update Loss = 0.2917\n",
      "\n",
      "--- Outer Iteration 44/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2917\n",
      "[Coordinate 13, Post-Update Loss = 0.2893\n",
      "\n",
      "--- Outer Iteration 45/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2893\n",
      "[Coordinate 0, Post-Update Loss = 0.2870\n",
      "\n",
      "--- Outer Iteration 46/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2870\n",
      "[Coordinate 13, Post-Update Loss = 0.2846\n",
      "\n",
      "--- Outer Iteration 47/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2846\n",
      "[Coordinate 13, Post-Update Loss = 0.2824\n",
      "\n",
      "--- Outer Iteration 48/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2824\n",
      "[Coordinate 1, Post-Update Loss = 0.2802\n",
      "\n",
      "--- Outer Iteration 49/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2802\n",
      "[Coordinate 1, Post-Update Loss = 0.2780\n",
      "\n",
      "--- Outer Iteration 50/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2780\n",
      "[Coordinate 13, Post-Update Loss = 0.2760\n",
      "\n",
      "--- Outer Iteration 51/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2760\n",
      "[Coordinate 0, Post-Update Loss = 0.2739\n",
      "\n",
      "--- Outer Iteration 52/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2739\n",
      "[Coordinate 13, Post-Update Loss = 0.2718\n",
      "\n",
      "--- Outer Iteration 53/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2718\n",
      "[Coordinate 13, Post-Update Loss = 0.2698\n",
      "\n",
      "--- Outer Iteration 54/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2698\n",
      "[Coordinate 1, Post-Update Loss = 0.2679\n",
      "\n",
      "--- Outer Iteration 55/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2679\n",
      "[Coordinate 0, Post-Update Loss = 0.2660\n",
      "\n",
      "--- Outer Iteration 56/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2660\n",
      "[Coordinate 13, Post-Update Loss = 0.2641\n",
      "\n",
      "--- Outer Iteration 57/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2641\n",
      "[Coordinate 13, Post-Update Loss = 0.2622\n",
      "\n",
      "--- Outer Iteration 58/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2622\n",
      "[Coordinate 1, Post-Update Loss = 0.2604\n",
      "\n",
      "--- Outer Iteration 59/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2604\n",
      "[Coordinate 13, Post-Update Loss = 0.2587\n",
      "\n",
      "--- Outer Iteration 60/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2587\n",
      "[Coordinate 0, Post-Update Loss = 0.2570\n",
      "\n",
      "--- Outer Iteration 61/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2570\n",
      "[Coordinate 13, Post-Update Loss = 0.2552\n",
      "\n",
      "--- Outer Iteration 62/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2552\n",
      "[Coordinate 13, Post-Update Loss = 0.2536\n",
      "\n",
      "--- Outer Iteration 63/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2536\n",
      "[Coordinate 1, Post-Update Loss = 0.2519\n",
      "\n",
      "--- Outer Iteration 64/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2519\n",
      "[Coordinate 1, Post-Update Loss = 0.2503\n",
      "\n",
      "--- Outer Iteration 65/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2503\n",
      "[Coordinate 13, Post-Update Loss = 0.2487\n",
      "\n",
      "--- Outer Iteration 66/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2487\n",
      "[Coordinate 0, Post-Update Loss = 0.2472\n",
      "\n",
      "--- Outer Iteration 67/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2472\n",
      "[Coordinate 13, Post-Update Loss = 0.2456\n",
      "\n",
      "--- Outer Iteration 68/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2456\n",
      "[Coordinate 13, Post-Update Loss = 0.2441\n",
      "\n",
      "--- Outer Iteration 69/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2441\n",
      "[Coordinate 1, Post-Update Loss = 0.2426\n",
      "\n",
      "--- Outer Iteration 70/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2426\n",
      "[Coordinate 1, Post-Update Loss = 0.2412\n",
      "\n",
      "--- Outer Iteration 71/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2412\n",
      "[Coordinate 13, Post-Update Loss = 0.2398\n",
      "\n",
      "--- Outer Iteration 72/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2398\n",
      "[Coordinate 0, Post-Update Loss = 0.2384\n",
      "\n",
      "--- Outer Iteration 73/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2384\n",
      "[Coordinate 13, Post-Update Loss = 0.2370\n",
      "\n",
      "--- Outer Iteration 74/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2370\n",
      "[Coordinate 13, Post-Update Loss = 0.2356\n",
      "\n",
      "--- Outer Iteration 75/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2356\n",
      "[Coordinate 1, Post-Update Loss = 0.2343\n",
      "\n",
      "--- Outer Iteration 76/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2343\n",
      "[Coordinate 13, Post-Update Loss = 0.2330\n",
      "\n",
      "--- Outer Iteration 77/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2330\n",
      "[Coordinate 0, Post-Update Loss = 0.2317\n",
      "\n",
      "--- Outer Iteration 78/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2317\n",
      "[Coordinate 13, Post-Update Loss = 0.2304\n",
      "\n",
      "--- Outer Iteration 79/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2304\n",
      "[Coordinate 13, Post-Update Loss = 0.2291\n",
      "\n",
      "--- Outer Iteration 80/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2291\n",
      "[Coordinate 1, Post-Update Loss = 0.2279\n",
      "\n",
      "--- Outer Iteration 81/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2279\n",
      "[Coordinate 1, Post-Update Loss = 0.2267\n",
      "\n",
      "--- Outer Iteration 82/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2267\n",
      "[Coordinate 13, Post-Update Loss = 0.2255\n",
      "\n",
      "--- Outer Iteration 83/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2255\n",
      "[Coordinate 0, Post-Update Loss = 0.2244\n",
      "\n",
      "--- Outer Iteration 84/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2244\n",
      "[Coordinate 13, Post-Update Loss = 0.2232\n",
      "\n",
      "--- Outer Iteration 85/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2232\n",
      "[Coordinate 13, Post-Update Loss = 0.2220\n",
      "\n",
      "--- Outer Iteration 86/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2220\n",
      "[Coordinate 1, Post-Update Loss = 0.2209\n",
      "\n",
      "--- Outer Iteration 87/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2209\n",
      "[Coordinate 1, Post-Update Loss = 0.2198\n",
      "\n",
      "--- Outer Iteration 88/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2198\n",
      "[Coordinate 13, Post-Update Loss = 0.2187\n",
      "\n",
      "--- Outer Iteration 89/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2187\n",
      "[Coordinate 0, Post-Update Loss = 0.2177\n",
      "\n",
      "--- Outer Iteration 90/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2177\n",
      "[Coordinate 13, Post-Update Loss = 0.2166\n",
      "\n",
      "--- Outer Iteration 91/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2166\n",
      "[Coordinate 13, Post-Update Loss = 0.2155\n",
      "\n",
      "--- Outer Iteration 92/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2155\n",
      "[Coordinate 1, Post-Update Loss = 0.2145\n",
      "\n",
      "--- Outer Iteration 93/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2145\n",
      "[Coordinate 1, Post-Update Loss = 0.2135\n",
      "\n",
      "--- Outer Iteration 94/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2135\n",
      "[Coordinate 13, Post-Update Loss = 0.2125\n",
      "\n",
      "--- Outer Iteration 95/100 ---\n",
      "[Coordinate 0, Pre-Update Loss = 0.2125\n",
      "[Coordinate 0, Post-Update Loss = 0.2115\n",
      "\n",
      "--- Outer Iteration 96/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2115\n",
      "[Coordinate 13, Post-Update Loss = 0.2105\n",
      "\n",
      "--- Outer Iteration 97/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2105\n",
      "[Coordinate 13, Post-Update Loss = 0.2096\n",
      "\n",
      "--- Outer Iteration 98/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2096\n",
      "[Coordinate 1, Post-Update Loss = 0.2086\n",
      "\n",
      "--- Outer Iteration 99/100 ---\n",
      "[Coordinate 1, Pre-Update Loss = 0.2086\n",
      "[Coordinate 1, Post-Update Loss = 0.2077\n",
      "\n",
      "--- Outer Iteration 100/100 ---\n",
      "[Coordinate 13, Pre-Update Loss = 0.2077\n",
      "[Coordinate 13, Post-Update Loss = 0.2068\n",
      "Final loss: 0.20677871772218862\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and preprocess data\n",
    "# ---------------------------\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "df['target'] = wine.target\n",
    "# Restrict to classes 0 and 1\n",
    "df = df[df['target'] < 2]\n",
    "\n",
    "X = df[wine.feature_names].values   # Shape: (N, d)\n",
    "y = df['target'].values            # Shape: (N,)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Normalize data so that each feature is in [-1, 1]\n",
    "#    X_norm[j] = 2 * (X[j] - min(X[j])) / (max(X[j]) - min(X[j])) - 1\n",
    "# ---------------------------\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_range = X_max - X_min\n",
    "\n",
    "# Safeguard against division by zero: if a feature is constant, keep it at 0\n",
    "X_range[X_range == 0] = 1e-9\n",
    "\n",
    "X_norm = 2.0 * (X - X_min) / X_range - 1.0\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Initialize weights using a simple line of best fit (LinearRegression)\n",
    "#    We'll do a linear regression on X_norm -> y, then store intercept + coefs.\n",
    "# ---------------------------\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_norm, y)\n",
    "\n",
    "# Extract coefficients and intercept\n",
    "# w_init will include intercept as w[0], followed by coefficients\n",
    "w_init = np.concatenate(([linreg.intercept_], linreg.coef_))\n",
    "\n",
    "# Augment X with a column of ones so that we can treat the intercept\n",
    "# as just another weight dimension.\n",
    "# New shape of X_aug: (N, d+1)\n",
    "X_aug = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define helper functions\n",
    "# ---------------------------\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The logistic sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(X_aug, y, w):\n",
    "    \"\"\"\n",
    "    Computes the average negative log-likelihood (logistic loss).\n",
    "    \"\"\"\n",
    "    eps = 1e-10  # For numerical stability\n",
    "    predictions = sigmoid(X_aug @ w)  # Shape: (N,)\n",
    "    N = X_aug.shape[0]\n",
    "    loss_val = -1.0 / N * np.sum(\n",
    "        y * np.log(predictions + eps) + (1 - y) * np.log(1 - predictions + eps)\n",
    "    )\n",
    "    return loss_val\n",
    "\n",
    "def compute_gradient(X_aug, y, w):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the negative log-likelihood (logistic loss).\n",
    "    X_aug: (N, d+1) with the first column all ones (for intercept).\n",
    "    y:     (N,) binary labels in {0, 1}.\n",
    "    w:     (d+1,) current weight vector (including intercept).\n",
    "    \n",
    "    Returns:\n",
    "       grad: (d+1,) gradient of logistic loss w.r.t. w\n",
    "    \"\"\"\n",
    "    N = X_aug.shape[0]\n",
    "    predictions = sigmoid(X_aug @ w)      # Shape: (N,)\n",
    "    grad = (1.0 / N) * (X_aug.T @ (predictions - y))\n",
    "    return grad\n",
    "\n",
    "def get_largest_gradient_dimension(X_aug, y, w):\n",
    "    \"\"\"\n",
    "    1. Compute the gradient for the current parameter w.\n",
    "    2. Find the index of the dimension with largest absolute partial derivative.\n",
    "    \n",
    "    Returns:\n",
    "       grad: the gradient (d+1,)\n",
    "       idx:  index of the largest (by absolute value) partial derivative\n",
    "    \"\"\"\n",
    "    grad = compute_gradient(X_aug, y, w)\n",
    "    idx = np.argmax(np.abs(grad))\n",
    "    return grad, idx\n",
    "\n",
    "def coordinate_descent_update(X_aug, y, w, idx, step_size, grad, num_steps=10):\n",
    "    \"\"\"\n",
    "    Performs repeated updates on a single coordinate (dimension) 'idx' with a fixed\n",
    "    step size. We re-compute the gradient each step because as we update w[idx],\n",
    "    the partial derivative changes.\n",
    "    \n",
    "    After each update, print the current loss.\n",
    "    \n",
    "    X_aug: (N, d+1) data\n",
    "    y:     (N,)     labels\n",
    "    w:     (d+1,)   weights (updated in place)\n",
    "    idx:   int      which dimension to update\n",
    "    step_size: float\n",
    "    num_steps: int   how many small updates to perform in that coordinate\n",
    "    \"\"\"\n",
    "    current_loss = logistic_loss(X_aug, y, w)\n",
    "    print(f\"[Coordinate {idx}, Pre-Update Loss = {current_loss:.4f}\")\n",
    "    for step in range(num_steps):\n",
    "        # Re-compute gradient\n",
    "        \n",
    "        \n",
    "        # Perform coordinate descent update in the negative gradient direction\n",
    "        w[idx] -= step_size * grad[idx]\n",
    "        \n",
    "        # Print loss after each update\n",
    "        new_loss = logistic_loss(X_aug, y, w)\n",
    "\n",
    "        if new_loss > current_loss:\n",
    "            w[idx] += grad[idx] * step_size\n",
    "            print(f\"Stopped Early after {step} steps\")\n",
    "            break\n",
    "        else:\n",
    "            current_loss = new_loss\n",
    "        \n",
    "    print(f\"[Coordinate {idx}, Post-Update Loss = {current_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Run the procedure\n",
    "# ---------------------------\n",
    "\n",
    "# Copy initial weights (so we don't overwrite w_init)\n",
    "w = w_init.copy()\n",
    "\n",
    "print(\"Initial loss:\", logistic_loss(X_aug, y, w))\n",
    "\n",
    "# Choose a fixed step size\n",
    "step_size = 0.01\n",
    "\n",
    "# Number of outer iterations over coordinates\n",
    "num_outer_iters = 100\n",
    "\n",
    "for iteration in range(num_outer_iters):\n",
    "    print(f\"\\n--- Outer Iteration {iteration+1}/{num_outer_iters} ---\")\n",
    "    # 1. Find the coordinate with largest partial derivative\n",
    "    grad, idx = get_largest_gradient_dimension(X_aug, y, w)\n",
    "    \n",
    "    # 2. Update that coordinate repeatedly\n",
    "    coordinate_descent_update(X_aug, y, w, idx, step_size, grad, num_steps=50)\n",
    "\n",
    "print(\"Final loss:\", logistic_loss(X_aug, y, w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Method: Full implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 0.5159\n",
      "\n",
      "--- Outer Iteration 1/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0015\n",
      "Pre-Update Loss = 0.5159\n",
      "Post-Update Loss = 0.4479\n",
      "\n",
      "--- Outer Iteration 2/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0012\n",
      "Pre-Update Loss = 0.4479\n",
      "Post-Update Loss = 0.3910\n",
      "\n",
      "--- Outer Iteration 3/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0011\n",
      "Pre-Update Loss = 0.3910\n",
      "Post-Update Loss = 0.3439\n",
      "\n",
      "--- Outer Iteration 4/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0009\n",
      "Pre-Update Loss = 0.3439\n",
      "Post-Update Loss = 0.3028\n",
      "\n",
      "--- Outer Iteration 5/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0007\n",
      "Pre-Update Loss = 0.3028\n",
      "Post-Update Loss = 0.2693\n",
      "\n",
      "--- Outer Iteration 6/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0006\n",
      "Pre-Update Loss = 0.2693\n",
      "Stopped Early after 42 steps\n",
      "Post-Update Loss = 0.2558\n",
      "\n",
      "--- Outer Iteration 7/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0008\n",
      "Pre-Update Loss = 0.2558\n",
      "Post-Update Loss = 0.2227\n",
      "\n",
      "--- Outer Iteration 8/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0006\n",
      "Pre-Update Loss = 0.2227\n",
      "Post-Update Loss = 0.1984\n",
      "\n",
      "--- Outer Iteration 9/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0004\n",
      "Pre-Update Loss = 0.1984\n",
      "Post-Update Loss = 0.1799\n",
      "\n",
      "--- Outer Iteration 10/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0004\n",
      "Pre-Update Loss = 0.1799\n",
      "Post-Update Loss = 0.1643\n",
      "\n",
      "--- Outer Iteration 11/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0004\n",
      "Pre-Update Loss = 0.1643\n",
      "Stopped Early after 39 steps\n",
      "Post-Update Loss = 0.1569\n",
      "\n",
      "--- Outer Iteration 12/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0003\n",
      "Pre-Update Loss = 0.1569\n",
      "Post-Update Loss = 0.1421\n",
      "\n",
      "--- Outer Iteration 13/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0002\n",
      "Pre-Update Loss = 0.1421\n",
      "Post-Update Loss = 0.1319\n",
      "\n",
      "--- Outer Iteration 14/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0002\n",
      "Pre-Update Loss = 0.1319\n",
      "Post-Update Loss = 0.1215\n",
      "\n",
      "--- Outer Iteration 15/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0002\n",
      "Pre-Update Loss = 0.1215\n",
      "Post-Update Loss = 0.1127\n",
      "\n",
      "--- Outer Iteration 16/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0002\n",
      "Pre-Update Loss = 0.1127\n",
      "Post-Update Loss = 0.1061\n",
      "\n",
      "--- Outer Iteration 17/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0002\n",
      "Pre-Update Loss = 0.1061\n",
      "Post-Update Loss = 0.0997\n",
      "\n",
      "--- Outer Iteration 18/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0002\n",
      "Pre-Update Loss = 0.0997\n",
      "Post-Update Loss = 0.0931\n",
      "\n",
      "--- Outer Iteration 19/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0931\n",
      "Post-Update Loss = 0.0872\n",
      "\n",
      "--- Outer Iteration 20/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0872\n",
      "Post-Update Loss = 0.0822\n",
      "\n",
      "--- Outer Iteration 21/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0822\n",
      "Post-Update Loss = 0.0774\n",
      "\n",
      "--- Outer Iteration 22/100 ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0774\n",
      "Post-Update Loss = 0.0728\n",
      "\n",
      "--- Outer Iteration 23/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0728\n",
      "Post-Update Loss = 0.0686\n",
      "\n",
      "--- Outer Iteration 24/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0686\n",
      "Post-Update Loss = 0.0651\n",
      "\n",
      "--- Outer Iteration 25/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0651\n",
      "Post-Update Loss = 0.0613\n",
      "\n",
      "--- Outer Iteration 26/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0613\n",
      "Post-Update Loss = 0.0582\n",
      "\n",
      "--- Outer Iteration 27/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0582\n",
      "Stopped Early after 22 steps\n",
      "Post-Update Loss = 0.0572\n",
      "\n",
      "--- Outer Iteration 28/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0572\n",
      "Post-Update Loss = 0.0546\n",
      "\n",
      "--- Outer Iteration 29/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0546\n",
      "Stopped Early after 26 steps\n",
      "Post-Update Loss = 0.0534\n",
      "\n",
      "--- Outer Iteration 30/100 ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0534\n",
      "Post-Update Loss = 0.0505\n",
      "\n",
      "--- Outer Iteration 31/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0505\n",
      "Post-Update Loss = 0.0480\n",
      "\n",
      "--- Outer Iteration 32/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0480\n",
      "Post-Update Loss = 0.0454\n",
      "\n",
      "--- Outer Iteration 33/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0454\n",
      "Post-Update Loss = 0.0430\n",
      "\n",
      "--- Outer Iteration 34/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0430\n",
      "Post-Update Loss = 0.0407\n",
      "\n",
      "--- Outer Iteration 35/100 ---\n",
      "Best coordinate: 11, direction: 1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0407\n",
      "Post-Update Loss = 0.0386\n",
      "\n",
      "--- Outer Iteration 36/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0386\n",
      "Stopped Early after 45 steps\n",
      "Post-Update Loss = 0.0375\n",
      "\n",
      "--- Outer Iteration 37/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0375\n",
      "Stopped Early after 22 steps\n",
      "Post-Update Loss = 0.0368\n",
      "\n",
      "--- Outer Iteration 38/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0001\n",
      "Pre-Update Loss = 0.0368\n",
      "Post-Update Loss = 0.0349\n",
      "\n",
      "--- Outer Iteration 39/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0349\n",
      "Post-Update Loss = 0.0331\n",
      "\n",
      "--- Outer Iteration 40/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0331\n",
      "Post-Update Loss = 0.0312\n",
      "\n",
      "--- Outer Iteration 41/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0312\n",
      "Post-Update Loss = 0.0295\n",
      "\n",
      "--- Outer Iteration 42/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0295\n",
      "Post-Update Loss = 0.0283\n",
      "\n",
      "--- Outer Iteration 43/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0283\n",
      "Post-Update Loss = 0.0270\n",
      "\n",
      "--- Outer Iteration 44/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0270\n",
      "Stopped Early after 45 steps\n",
      "Post-Update Loss = 0.0261\n",
      "\n",
      "--- Outer Iteration 45/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0261\n",
      "Post-Update Loss = 0.0248\n",
      "\n",
      "--- Outer Iteration 46/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0248\n",
      "Post-Update Loss = 0.0235\n",
      "\n",
      "--- Outer Iteration 47/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0235\n",
      "Post-Update Loss = 0.0224\n",
      "\n",
      "--- Outer Iteration 48/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0224\n",
      "Stopped Early after 21 steps\n",
      "Post-Update Loss = 0.0220\n",
      "\n",
      "--- Outer Iteration 49/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0220\n",
      "Post-Update Loss = 0.0210\n",
      "\n",
      "--- Outer Iteration 50/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0210\n",
      "Post-Update Loss = 0.0200\n",
      "\n",
      "--- Outer Iteration 51/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0200\n",
      "Post-Update Loss = 0.0189\n",
      "\n",
      "--- Outer Iteration 52/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0189\n",
      "Post-Update Loss = 0.0181\n",
      "\n",
      "--- Outer Iteration 53/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0181\n",
      "Post-Update Loss = 0.0172\n",
      "\n",
      "--- Outer Iteration 54/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0172\n",
      "Stopped Early after 38 steps\n",
      "Post-Update Loss = 0.0168\n",
      "\n",
      "--- Outer Iteration 55/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0168\n",
      "Stopped Early after 15 steps\n",
      "Post-Update Loss = 0.0166\n",
      "\n",
      "--- Outer Iteration 56/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0166\n",
      "Post-Update Loss = 0.0159\n",
      "\n",
      "--- Outer Iteration 57/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0159\n",
      "Post-Update Loss = 0.0152\n",
      "\n",
      "--- Outer Iteration 58/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0152\n",
      "Post-Update Loss = 0.0144\n",
      "\n",
      "--- Outer Iteration 59/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0144\n",
      "Post-Update Loss = 0.0137\n",
      "\n",
      "--- Outer Iteration 60/100 ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0137\n",
      "Post-Update Loss = 0.0132\n",
      "\n",
      "--- Outer Iteration 61/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0132\n",
      "Stopped Early after 20 steps\n",
      "Post-Update Loss = 0.0129\n",
      "\n",
      "--- Outer Iteration 62/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0129\n",
      "Post-Update Loss = 0.0124\n",
      "\n",
      "--- Outer Iteration 63/100 ---\n",
      "Best coordinate: 11, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0124\n",
      "Post-Update Loss = 0.0119\n",
      "\n",
      "--- Outer Iteration 64/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0119\n",
      "Post-Update Loss = 0.0114\n",
      "\n",
      "--- Outer Iteration 65/100 ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0114\n",
      "Post-Update Loss = 0.0109\n",
      "\n",
      "--- Outer Iteration 66/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0109\n",
      "Post-Update Loss = 0.0105\n",
      "\n",
      "--- Outer Iteration 67/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0105\n",
      "Post-Update Loss = 0.0100\n",
      "\n",
      "--- Outer Iteration 68/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0100\n",
      "Post-Update Loss = 0.0095\n",
      "\n",
      "--- Outer Iteration 69/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0095\n",
      "Post-Update Loss = 0.0092\n",
      "\n",
      "--- Outer Iteration 70/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0092\n",
      "Post-Update Loss = 0.0088\n",
      "\n",
      "--- Outer Iteration 71/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0088\n",
      "Post-Update Loss = 0.0084\n",
      "\n",
      "--- Outer Iteration 72/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0084\n",
      "Stopped Early after 30 steps\n",
      "Post-Update Loss = 0.0082\n",
      "\n",
      "--- Outer Iteration 73/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0082\n",
      "Stopped Early after 21 steps\n",
      "Post-Update Loss = 0.0081\n",
      "\n",
      "--- Outer Iteration 74/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0081\n",
      "Post-Update Loss = 0.0077\n",
      "\n",
      "--- Outer Iteration 75/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0077\n",
      "Post-Update Loss = 0.0074\n",
      "\n",
      "--- Outer Iteration 76/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0074\n",
      "Post-Update Loss = 0.0072\n",
      "\n",
      "--- Outer Iteration 77/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0072\n",
      "Post-Update Loss = 0.0068\n",
      "\n",
      "--- Outer Iteration 78/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0068\n",
      "Stopped Early after 33 steps\n",
      "Post-Update Loss = 0.0067\n",
      "\n",
      "--- Outer Iteration 79/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0067\n",
      "Stopped Early after 14 steps\n",
      "Post-Update Loss = 0.0066\n",
      "\n",
      "--- Outer Iteration 80/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0066\n",
      "Post-Update Loss = 0.0064\n",
      "\n",
      "--- Outer Iteration 81/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0064\n",
      "Stopped Early after 13 steps\n",
      "Post-Update Loss = 0.0063\n",
      "\n",
      "--- Outer Iteration 82/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0063\n",
      "Post-Update Loss = 0.0060\n",
      "\n",
      "--- Outer Iteration 83/100 ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0060\n",
      "Post-Update Loss = 0.0058\n",
      "\n",
      "--- Outer Iteration 84/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0058\n",
      "Stopped Early after 17 steps\n",
      "Post-Update Loss = 0.0057\n",
      "\n",
      "--- Outer Iteration 85/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0057\n",
      "Post-Update Loss = 0.0055\n",
      "\n",
      "--- Outer Iteration 86/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0055\n",
      "Post-Update Loss = 0.0054\n",
      "\n",
      "--- Outer Iteration 87/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0054\n",
      "Post-Update Loss = 0.0051\n",
      "\n",
      "--- Outer Iteration 88/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0051\n",
      "Stopped Early after 30 steps\n",
      "Post-Update Loss = 0.0050\n",
      "\n",
      "--- Outer Iteration 89/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0050\n",
      "Stopped Early after 18 steps\n",
      "Post-Update Loss = 0.0049\n",
      "\n",
      "--- Outer Iteration 90/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0049\n",
      "Post-Update Loss = 0.0047\n",
      "\n",
      "--- Outer Iteration 91/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0047\n",
      "Stopped Early after 13 steps\n",
      "Post-Update Loss = 0.0047\n",
      "\n",
      "--- Outer Iteration 92/100 ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0047\n",
      "Stopped Early after 30 steps\n",
      "Post-Update Loss = 0.0046\n",
      "\n",
      "--- Outer Iteration 93/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0046\n",
      "Stopped Early after 15 steps\n",
      "Post-Update Loss = 0.0046\n",
      "\n",
      "--- Outer Iteration 94/100 ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0046\n",
      "Post-Update Loss = 0.0044\n",
      "\n",
      "--- Outer Iteration 95/100 ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0044\n",
      "Post-Update Loss = 0.0043\n",
      "\n",
      "--- Outer Iteration 96/100 ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0043\n",
      "Stopped Early after 16 steps\n",
      "Post-Update Loss = 0.0042\n",
      "\n",
      "--- Outer Iteration 97/100 ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0042\n",
      "Post-Update Loss = 0.0041\n",
      "\n",
      "--- Outer Iteration 98/100 ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0041\n",
      "Post-Update Loss = 0.0039\n",
      "\n",
      "--- Outer Iteration 99/100 ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0039\n",
      "Post-Update Loss = 0.0037\n",
      "\n",
      "--- Outer Iteration 100/100 ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "Pre-Update Loss = 0.0037\n",
      "Post-Update Loss = 0.0036\n",
      "\n",
      "Final weights (including intercept): [-2.86976086e+00 -6.81537323e+00 -3.10736894e+00 -6.82243847e+00\n",
      "  7.33232851e+00 -2.39452650e-03  9.75980033e-02 -1.31221972e-01\n",
      "  1.79888263e-02  6.60927171e-02 -3.03239507e+00  1.01948310e+00\n",
      " -4.18894639e+00 -1.04095673e+01]\n",
      "Final Loss: 0.0036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and preprocess data\n",
    "# ---------------------------\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "df['target'] = wine.target\n",
    "# Restrict to classes 0 and 1\n",
    "df = df[df['target'] < 2]\n",
    "\n",
    "X = df[wine.feature_names].values   # Shape: (N, d)\n",
    "y = df['target'].values            # Shape: (N,)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Normalize each feature to be in [-1, 1]\n",
    "#    X_norm[j] = 2 * (X[j] - min(X[j])) / (max(X[j]) - min(X[j])) - 1\n",
    "# ---------------------------\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_range = X_max - X_min\n",
    "\n",
    "# Safeguard: If a feature is constant, replace zero range by small value\n",
    "X_range[X_range == 0] = 1e-9\n",
    "\n",
    "X_norm = 2.0 * (X - X_min) / X_range - 1.0\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Initialize weights using a simple line of best fit (LinearRegression)\n",
    "#    We'll fit linear regression on X_norm -> y, then extract intercept + coefs.\n",
    "# ---------------------------\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_norm, y)\n",
    "\n",
    "# w_init will include intercept as w[0], followed by coefficients\n",
    "w_init = np.concatenate(([linreg.intercept_], linreg.coef_))\n",
    "\n",
    "# Augment X with a column of ones (for the intercept),\n",
    "# so X_aug has shape (N, d+1).\n",
    "X_aug = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Define helper functions\n",
    "# ---------------------------\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Logistic sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(X_aug, y, w):\n",
    "    \"\"\"\n",
    "    Computes the average negative log-likelihood (logistic loss).\n",
    "    \"\"\"\n",
    "    eps = 1e-10  # For numerical stability\n",
    "    predictions = sigmoid(X_aug @ w)  # Shape: (N,)\n",
    "    N = X_aug.shape[0]\n",
    "    loss_val = -1.0 / N * np.sum(\n",
    "        y * np.log(predictions + eps) + (1 - y) * np.log(1 - predictions + eps)\n",
    "    )\n",
    "    return loss_val\n",
    "\n",
    "def find_best_coordinate_and_direction(X_aug, y, w, step_size):\n",
    "    \"\"\"\n",
    "    For each coordinate i, we try moving w[i] + step_size and w[i] - step_size.\n",
    "    We see how much each move decreases the logistic loss.\n",
    "\n",
    "    We pick the coordinate and direction (+/-) that yields the greatest\n",
    "    DECREASE in the loss.\n",
    "\n",
    "    Returns:\n",
    "      best_coord:    The index (0..d) that yields the largest improvement\n",
    "      best_direction: +1 or -1, indicating which direction is best\n",
    "      best_improvement: The amount of loss decrease achieved by that coordinate/direction\n",
    "    \"\"\"\n",
    "    current_loss = logistic_loss(X_aug, y, w)\n",
    "    best_coord = 0\n",
    "    best_direction = 0\n",
    "    best_improvement = 0.0\n",
    "\n",
    "    d_plus_one = len(w)\n",
    "\n",
    "    for i in range(d_plus_one):\n",
    "        # Try w[i] + step_size\n",
    "        w_temp_plus = w.copy()\n",
    "        w_temp_plus[i] += step_size\n",
    "        loss_plus = logistic_loss(X_aug, y, w_temp_plus)\n",
    "        improvement_plus = current_loss - loss_plus  # positive if the loss went down\n",
    "\n",
    "        # Try w[i] - step_size\n",
    "        w_temp_minus = w.copy()\n",
    "        w_temp_minus[i] -= step_size\n",
    "        loss_minus = logistic_loss(X_aug, y, w_temp_minus)\n",
    "        improvement_minus = current_loss - loss_minus\n",
    "\n",
    "        # Determine which direction yields the larger improvement for this coordinate\n",
    "        if improvement_plus > improvement_minus:\n",
    "            improvement_for_i = improvement_plus\n",
    "            direction_for_i = +1\n",
    "        else:\n",
    "            improvement_for_i = improvement_minus\n",
    "            direction_for_i = -1\n",
    "\n",
    "        # Check if this is better than our current best\n",
    "        if improvement_for_i > best_improvement:\n",
    "            best_improvement = improvement_for_i\n",
    "            best_coord = i\n",
    "            best_direction = direction_for_i\n",
    "\n",
    "    return best_coord, best_direction, best_improvement\n",
    "\n",
    "def coordinate_descent_update(X_aug, y, w, coord, direction, step_size, num_steps=5):\n",
    "    \"\"\"\n",
    "    Once we've chosen which coordinate and direction yields the best\n",
    "    immediate improvement, we can do multiple steps in that direction.\n",
    "\n",
    "    For each step, we update w[coord] by 'direction * step_size'\n",
    "    and print the new loss.\n",
    "    \"\"\"\n",
    "    curr_loss = logistic_loss(X_aug, y, w)\n",
    "    print(f\"Pre-Update Loss = {curr_loss:.4f}\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        w[coord] += direction * step_size\n",
    "        new_loss = logistic_loss(X_aug, y, w)\n",
    "        if new_loss > curr_loss:\n",
    "            w[coord] -= direction * step_size\n",
    "            print(f\"Stopped Early after {step} steps\")\n",
    "            break\n",
    "        else:\n",
    "            curr_loss = new_loss\n",
    "    print(f\"Post-Update Loss = {logistic_loss(X_aug, y, w):.4f}\")\n",
    "        \n",
    "\n",
    "# ---------------------------\n",
    "# 5. Run the procedure\n",
    "# ---------------------------\n",
    "w = w_init.copy()\n",
    "initial_loss = logistic_loss(X_aug, y, w)\n",
    "\n",
    "print(f\"Initial Loss: {initial_loss:.4f}\")\n",
    "\n",
    "num_outer_iters = 100  # Number of times we scan and pick the best coordinate\n",
    "step_size = 0.01      # Fixed step size\n",
    "num_steps = 50        # How many times we move in that chosen direction once found\n",
    "\n",
    "for iteration in range(num_outer_iters):\n",
    "    print(f\"\\n--- Outer Iteration {iteration+1}/{num_outer_iters} ---\")\n",
    "    best_coord, best_direction, best_improvement = find_best_coordinate_and_direction(X_aug, y, w, step_size)\n",
    "\n",
    "    if best_improvement <= 0:\n",
    "        # No improvement found by any coordinate update (could happen if we're at a local minimum or step is too large)\n",
    "        print(\"No further improvement found. Stopping early.\")\n",
    "        break\n",
    "\n",
    "    # Otherwise, do multiple steps in that chosen coordinate and direction\n",
    "    print(f\"Best coordinate: {best_coord}, direction: {best_direction}, improvement: {best_improvement:.4f}\")\n",
    "    coordinate_descent_update(X_aug, y, w, best_coord, best_direction, step_size, num_steps=num_steps)\n",
    "\n",
    "# Final result\n",
    "final_loss = logistic_loss(X_aug, y, w)\n",
    "print(\"\\nFinal weights (including intercept):\", w)\n",
    "print(f\"Final Loss: {final_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Comparison Between Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Method 1: Best Coordinate Descent ===\n",
      "Initial Loss (Method 1): 0.5159\n",
      "\n",
      "--- Outer Iteration 1/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0015\n",
      "   Pre-Update Loss = 0.5159\n",
      "   Post-Update Loss = 0.4479\n",
      "\n",
      "--- Outer Iteration 2/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0012\n",
      "   Pre-Update Loss = 0.4479\n",
      "   Post-Update Loss = 0.3910\n",
      "\n",
      "--- Outer Iteration 3/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0011\n",
      "   Pre-Update Loss = 0.3910\n",
      "   Post-Update Loss = 0.3439\n",
      "\n",
      "--- Outer Iteration 4/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0009\n",
      "   Pre-Update Loss = 0.3439\n",
      "   Post-Update Loss = 0.3028\n",
      "\n",
      "--- Outer Iteration 5/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0007\n",
      "   Pre-Update Loss = 0.3028\n",
      "   Post-Update Loss = 0.2693\n",
      "\n",
      "--- Outer Iteration 6/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0006\n",
      "   Pre-Update Loss = 0.2693\n",
      "   Stopped Early after 42 steps (no improvement).\n",
      "   Post-Update Loss = 0.2558\n",
      "\n",
      "--- Outer Iteration 7/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0008\n",
      "   Pre-Update Loss = 0.2558\n",
      "   Post-Update Loss = 0.2227\n",
      "\n",
      "--- Outer Iteration 8/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0006\n",
      "   Pre-Update Loss = 0.2227\n",
      "   Post-Update Loss = 0.1984\n",
      "\n",
      "--- Outer Iteration 9/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0004\n",
      "   Pre-Update Loss = 0.1984\n",
      "   Post-Update Loss = 0.1799\n",
      "\n",
      "--- Outer Iteration 10/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0004\n",
      "   Pre-Update Loss = 0.1799\n",
      "   Post-Update Loss = 0.1643\n",
      "\n",
      "--- Outer Iteration 11/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0004\n",
      "   Pre-Update Loss = 0.1643\n",
      "   Stopped Early after 39 steps (no improvement).\n",
      "   Post-Update Loss = 0.1569\n",
      "\n",
      "--- Outer Iteration 12/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0003\n",
      "   Pre-Update Loss = 0.1569\n",
      "   Post-Update Loss = 0.1421\n",
      "\n",
      "--- Outer Iteration 13/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0002\n",
      "   Pre-Update Loss = 0.1421\n",
      "   Post-Update Loss = 0.1319\n",
      "\n",
      "--- Outer Iteration 14/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0002\n",
      "   Pre-Update Loss = 0.1319\n",
      "   Post-Update Loss = 0.1215\n",
      "\n",
      "--- Outer Iteration 15/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0002\n",
      "   Pre-Update Loss = 0.1215\n",
      "   Post-Update Loss = 0.1127\n",
      "\n",
      "--- Outer Iteration 16/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0002\n",
      "   Pre-Update Loss = 0.1127\n",
      "   Post-Update Loss = 0.1061\n",
      "\n",
      "--- Outer Iteration 17/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0002\n",
      "   Pre-Update Loss = 0.1061\n",
      "   Post-Update Loss = 0.0997\n",
      "\n",
      "--- Outer Iteration 18/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0002\n",
      "   Pre-Update Loss = 0.0997\n",
      "   Post-Update Loss = 0.0931\n",
      "\n",
      "--- Outer Iteration 19/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0931\n",
      "   Post-Update Loss = 0.0872\n",
      "\n",
      "--- Outer Iteration 20/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0872\n",
      "   Post-Update Loss = 0.0822\n",
      "\n",
      "--- Outer Iteration 21/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0822\n",
      "   Post-Update Loss = 0.0774\n",
      "\n",
      "--- Outer Iteration 22/100 (Method 1) ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0774\n",
      "   Post-Update Loss = 0.0728\n",
      "\n",
      "--- Outer Iteration 23/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0728\n",
      "   Post-Update Loss = 0.0686\n",
      "\n",
      "--- Outer Iteration 24/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0686\n",
      "   Post-Update Loss = 0.0651\n",
      "\n",
      "--- Outer Iteration 25/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0651\n",
      "   Post-Update Loss = 0.0613\n",
      "\n",
      "--- Outer Iteration 26/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0613\n",
      "   Post-Update Loss = 0.0582\n",
      "\n",
      "--- Outer Iteration 27/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0582\n",
      "   Stopped Early after 22 steps (no improvement).\n",
      "   Post-Update Loss = 0.0572\n",
      "\n",
      "--- Outer Iteration 28/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0572\n",
      "   Post-Update Loss = 0.0546\n",
      "\n",
      "--- Outer Iteration 29/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0546\n",
      "   Stopped Early after 26 steps (no improvement).\n",
      "   Post-Update Loss = 0.0534\n",
      "\n",
      "--- Outer Iteration 30/100 (Method 1) ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0534\n",
      "   Post-Update Loss = 0.0505\n",
      "\n",
      "--- Outer Iteration 31/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0505\n",
      "   Post-Update Loss = 0.0480\n",
      "\n",
      "--- Outer Iteration 32/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0480\n",
      "   Post-Update Loss = 0.0454\n",
      "\n",
      "--- Outer Iteration 33/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0454\n",
      "   Post-Update Loss = 0.0430\n",
      "\n",
      "--- Outer Iteration 34/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0430\n",
      "   Post-Update Loss = 0.0407\n",
      "\n",
      "--- Outer Iteration 35/100 (Method 1) ---\n",
      "Best coordinate: 11, direction: 1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0407\n",
      "   Post-Update Loss = 0.0386\n",
      "\n",
      "--- Outer Iteration 36/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0386\n",
      "   Stopped Early after 45 steps (no improvement).\n",
      "   Post-Update Loss = 0.0375\n",
      "\n",
      "--- Outer Iteration 37/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0375\n",
      "   Stopped Early after 22 steps (no improvement).\n",
      "   Post-Update Loss = 0.0368\n",
      "\n",
      "--- Outer Iteration 38/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0001\n",
      "   Pre-Update Loss = 0.0368\n",
      "   Post-Update Loss = 0.0349\n",
      "\n",
      "--- Outer Iteration 39/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0349\n",
      "   Post-Update Loss = 0.0331\n",
      "\n",
      "--- Outer Iteration 40/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0331\n",
      "   Post-Update Loss = 0.0312\n",
      "\n",
      "--- Outer Iteration 41/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0312\n",
      "   Post-Update Loss = 0.0295\n",
      "\n",
      "--- Outer Iteration 42/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0295\n",
      "   Post-Update Loss = 0.0283\n",
      "\n",
      "--- Outer Iteration 43/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0283\n",
      "   Post-Update Loss = 0.0270\n",
      "\n",
      "--- Outer Iteration 44/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0270\n",
      "   Stopped Early after 45 steps (no improvement).\n",
      "   Post-Update Loss = 0.0261\n",
      "\n",
      "--- Outer Iteration 45/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0261\n",
      "   Post-Update Loss = 0.0248\n",
      "\n",
      "--- Outer Iteration 46/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0248\n",
      "   Post-Update Loss = 0.0235\n",
      "\n",
      "--- Outer Iteration 47/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0235\n",
      "   Post-Update Loss = 0.0224\n",
      "\n",
      "--- Outer Iteration 48/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0224\n",
      "   Stopped Early after 21 steps (no improvement).\n",
      "   Post-Update Loss = 0.0220\n",
      "\n",
      "--- Outer Iteration 49/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0220\n",
      "   Post-Update Loss = 0.0210\n",
      "\n",
      "--- Outer Iteration 50/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0210\n",
      "   Post-Update Loss = 0.0200\n",
      "\n",
      "--- Outer Iteration 51/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0200\n",
      "   Post-Update Loss = 0.0189\n",
      "\n",
      "--- Outer Iteration 52/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0189\n",
      "   Post-Update Loss = 0.0181\n",
      "\n",
      "--- Outer Iteration 53/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0181\n",
      "   Post-Update Loss = 0.0172\n",
      "\n",
      "--- Outer Iteration 54/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0172\n",
      "   Stopped Early after 38 steps (no improvement).\n",
      "   Post-Update Loss = 0.0168\n",
      "\n",
      "--- Outer Iteration 55/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0168\n",
      "   Stopped Early after 15 steps (no improvement).\n",
      "   Post-Update Loss = 0.0166\n",
      "\n",
      "--- Outer Iteration 56/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0166\n",
      "   Post-Update Loss = 0.0159\n",
      "\n",
      "--- Outer Iteration 57/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0159\n",
      "   Post-Update Loss = 0.0152\n",
      "\n",
      "--- Outer Iteration 58/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0152\n",
      "   Post-Update Loss = 0.0144\n",
      "\n",
      "--- Outer Iteration 59/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0144\n",
      "   Post-Update Loss = 0.0137\n",
      "\n",
      "--- Outer Iteration 60/100 (Method 1) ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0137\n",
      "   Post-Update Loss = 0.0132\n",
      "\n",
      "--- Outer Iteration 61/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0132\n",
      "   Stopped Early after 20 steps (no improvement).\n",
      "   Post-Update Loss = 0.0129\n",
      "\n",
      "--- Outer Iteration 62/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0129\n",
      "   Post-Update Loss = 0.0124\n",
      "\n",
      "--- Outer Iteration 63/100 (Method 1) ---\n",
      "Best coordinate: 11, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0124\n",
      "   Post-Update Loss = 0.0119\n",
      "\n",
      "--- Outer Iteration 64/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0119\n",
      "   Post-Update Loss = 0.0114\n",
      "\n",
      "--- Outer Iteration 65/100 (Method 1) ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0114\n",
      "   Post-Update Loss = 0.0109\n",
      "\n",
      "--- Outer Iteration 66/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0109\n",
      "   Post-Update Loss = 0.0105\n",
      "\n",
      "--- Outer Iteration 67/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0105\n",
      "   Post-Update Loss = 0.0100\n",
      "\n",
      "--- Outer Iteration 68/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0100\n",
      "   Post-Update Loss = 0.0095\n",
      "\n",
      "--- Outer Iteration 69/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0095\n",
      "   Post-Update Loss = 0.0092\n",
      "\n",
      "--- Outer Iteration 70/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0092\n",
      "   Post-Update Loss = 0.0088\n",
      "\n",
      "--- Outer Iteration 71/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0088\n",
      "   Post-Update Loss = 0.0084\n",
      "\n",
      "--- Outer Iteration 72/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0084\n",
      "   Stopped Early after 30 steps (no improvement).\n",
      "   Post-Update Loss = 0.0082\n",
      "\n",
      "--- Outer Iteration 73/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0082\n",
      "   Stopped Early after 21 steps (no improvement).\n",
      "   Post-Update Loss = 0.0081\n",
      "\n",
      "--- Outer Iteration 74/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0081\n",
      "   Post-Update Loss = 0.0077\n",
      "\n",
      "--- Outer Iteration 75/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0077\n",
      "   Post-Update Loss = 0.0074\n",
      "\n",
      "--- Outer Iteration 76/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0074\n",
      "   Post-Update Loss = 0.0072\n",
      "\n",
      "--- Outer Iteration 77/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0072\n",
      "   Post-Update Loss = 0.0068\n",
      "\n",
      "--- Outer Iteration 78/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0068\n",
      "   Stopped Early after 33 steps (no improvement).\n",
      "   Post-Update Loss = 0.0067\n",
      "\n",
      "--- Outer Iteration 79/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0067\n",
      "   Stopped Early after 14 steps (no improvement).\n",
      "   Post-Update Loss = 0.0066\n",
      "\n",
      "--- Outer Iteration 80/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0066\n",
      "   Post-Update Loss = 0.0064\n",
      "\n",
      "--- Outer Iteration 81/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0064\n",
      "   Stopped Early after 13 steps (no improvement).\n",
      "   Post-Update Loss = 0.0063\n",
      "\n",
      "--- Outer Iteration 82/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0063\n",
      "   Post-Update Loss = 0.0060\n",
      "\n",
      "--- Outer Iteration 83/100 (Method 1) ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0060\n",
      "   Post-Update Loss = 0.0058\n",
      "\n",
      "--- Outer Iteration 84/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0058\n",
      "   Stopped Early after 17 steps (no improvement).\n",
      "   Post-Update Loss = 0.0057\n",
      "\n",
      "--- Outer Iteration 85/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0057\n",
      "   Post-Update Loss = 0.0055\n",
      "\n",
      "--- Outer Iteration 86/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0055\n",
      "   Post-Update Loss = 0.0054\n",
      "\n",
      "--- Outer Iteration 87/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0054\n",
      "   Post-Update Loss = 0.0051\n",
      "\n",
      "--- Outer Iteration 88/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0051\n",
      "   Stopped Early after 30 steps (no improvement).\n",
      "   Post-Update Loss = 0.0050\n",
      "\n",
      "--- Outer Iteration 89/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0050\n",
      "   Stopped Early after 18 steps (no improvement).\n",
      "   Post-Update Loss = 0.0049\n",
      "\n",
      "--- Outer Iteration 90/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0049\n",
      "   Post-Update Loss = 0.0047\n",
      "\n",
      "--- Outer Iteration 91/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0047\n",
      "   Stopped Early after 13 steps (no improvement).\n",
      "   Post-Update Loss = 0.0047\n",
      "\n",
      "--- Outer Iteration 92/100 (Method 1) ---\n",
      "Best coordinate: 2, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0047\n",
      "   Stopped Early after 30 steps (no improvement).\n",
      "   Post-Update Loss = 0.0046\n",
      "\n",
      "--- Outer Iteration 93/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0046\n",
      "   Stopped Early after 15 steps (no improvement).\n",
      "   Post-Update Loss = 0.0046\n",
      "\n",
      "--- Outer Iteration 94/100 (Method 1) ---\n",
      "Best coordinate: 1, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0046\n",
      "   Post-Update Loss = 0.0044\n",
      "\n",
      "--- Outer Iteration 95/100 (Method 1) ---\n",
      "Best coordinate: 10, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0044\n",
      "   Post-Update Loss = 0.0043\n",
      "\n",
      "--- Outer Iteration 96/100 (Method 1) ---\n",
      "Best coordinate: 0, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0043\n",
      "   Stopped Early after 16 steps (no improvement).\n",
      "   Post-Update Loss = 0.0042\n",
      "\n",
      "--- Outer Iteration 97/100 (Method 1) ---\n",
      "Best coordinate: 13, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0042\n",
      "   Post-Update Loss = 0.0041\n",
      "\n",
      "--- Outer Iteration 98/100 (Method 1) ---\n",
      "Best coordinate: 3, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0041\n",
      "   Post-Update Loss = 0.0039\n",
      "\n",
      "--- Outer Iteration 99/100 (Method 1) ---\n",
      "Best coordinate: 4, direction: 1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0039\n",
      "   Post-Update Loss = 0.0037\n",
      "\n",
      "--- Outer Iteration 100/100 (Method 1) ---\n",
      "Best coordinate: 12, direction: -1, improvement: 0.0000\n",
      "   Pre-Update Loss = 0.0037\n",
      "   Post-Update Loss = 0.0036\n",
      "\n",
      "Final weights (Method 1): [-2.86976086e+00 -6.81537323e+00 -3.10736894e+00 -6.82243847e+00\n",
      "  7.33232851e+00 -2.39452650e-03  9.75980033e-02 -1.31221972e-01\n",
      "  1.79888263e-02  6.60927171e-02 -3.03239507e+00  1.01948310e+00\n",
      " -4.18894639e+00 -1.04095673e+01]\n",
      "Final Loss (Method 1): 0.0036\n",
      "\n",
      "=== Method 2: Random Coordinate Descent ===\n",
      "Initial Loss (Method 2): 0.5159\n",
      "\n",
      "--- Outer Iteration 1/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000697\n",
      "   Pre-Update Loss = 0.5159\n",
      "   Post-Update Loss = 0.4857\n",
      "\n",
      "--- Outer Iteration 2/100 (Method 2) ---\n",
      "Random coordinate: 2, direction: 1, improvement: 0.000286\n",
      "   Pre-Update Loss = 0.4857\n",
      "   Stopped Early after 34 steps (no improvement).\n",
      "   Post-Update Loss = 0.4808\n",
      "\n",
      "--- Outer Iteration 3/100 (Method 2) ---\n",
      "Random coordinate: 8, direction: 1, improvement: 0.000493\n",
      "   Pre-Update Loss = 0.4808\n",
      "   Post-Update Loss = 0.4624\n",
      "\n",
      "--- Outer Iteration 4/100 (Method 2) ---\n",
      "Random coordinate: 7, direction: -1, improvement: 0.000641\n",
      "   Pre-Update Loss = 0.4624\n",
      "   Post-Update Loss = 0.4335\n",
      "\n",
      "--- Outer Iteration 5/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000980\n",
      "   Pre-Update Loss = 0.4335\n",
      "   Post-Update Loss = 0.3901\n",
      "\n",
      "--- Outer Iteration 6/100 (Method 2) ---\n",
      "Random coordinate: 11, direction: 1, improvement: 0.000071\n",
      "   Pre-Update Loss = 0.3901\n",
      "   Stopped Early after 21 steps (no improvement).\n",
      "   Post-Update Loss = 0.3893\n",
      "\n",
      "--- Outer Iteration 7/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: -1, improvement: 0.000303\n",
      "   Pre-Update Loss = 0.3893\n",
      "   Post-Update Loss = 0.3796\n",
      "\n",
      "--- Outer Iteration 8/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: -1, improvement: 0.000087\n",
      "   Pre-Update Loss = 0.3796\n",
      "   Stopped Early after 22 steps (no improvement).\n",
      "   Post-Update Loss = 0.3786\n",
      "\n",
      "--- Outer Iteration 9/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: -1, improvement: 0.000430\n",
      "   Pre-Update Loss = 0.3786\n",
      "   Post-Update Loss = 0.3601\n",
      "\n",
      "--- Outer Iteration 10/100 (Method 2) ---\n",
      "Random coordinate: 7, direction: -1, improvement: 0.000263\n",
      "   Pre-Update Loss = 0.3601\n",
      "   Post-Update Loss = 0.3489\n",
      "\n",
      "--- Outer Iteration 11/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000299\n",
      "   Pre-Update Loss = 0.3489\n",
      "   Post-Update Loss = 0.3361\n",
      "\n",
      "--- Outer Iteration 12/100 (Method 2) ---\n",
      "Random coordinate: 1, direction: -1, improvement: 0.000947\n",
      "   Pre-Update Loss = 0.3361\n",
      "   Post-Update Loss = 0.2929\n",
      "\n",
      "--- Outer Iteration 13/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000213\n",
      "   Pre-Update Loss = 0.2929\n",
      "   Post-Update Loss = 0.2851\n",
      "\n",
      "--- Outer Iteration 14/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000765\n",
      "   Pre-Update Loss = 0.2851\n",
      "   Post-Update Loss = 0.2514\n",
      "\n",
      "--- Outer Iteration 15/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: 1, improvement: 0.000061\n",
      "   Pre-Update Loss = 0.2514\n",
      "   Stopped Early after 22 steps (no improvement).\n",
      "   Post-Update Loss = 0.2507\n",
      "\n",
      "--- Outer Iteration 16/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: -1, improvement: 0.000105\n",
      "   Pre-Update Loss = 0.2507\n",
      "   Post-Update Loss = 0.2473\n",
      "\n",
      "--- Outer Iteration 17/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: -1, improvement: 0.000033\n",
      "   Pre-Update Loss = 0.2473\n",
      "   Stopped Early after 26 steps (no improvement).\n",
      "   Post-Update Loss = 0.2468\n",
      "\n",
      "--- Outer Iteration 18/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000603\n",
      "   Pre-Update Loss = 0.2468\n",
      "   Post-Update Loss = 0.2203\n",
      "\n",
      "--- Outer Iteration 19/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000120\n",
      "   Pre-Update Loss = 0.2203\n",
      "   Post-Update Loss = 0.2158\n",
      "\n",
      "--- Outer Iteration 20/100 (Method 2) ---\n",
      "Random coordinate: 2, direction: 1, improvement: 0.000046\n",
      "   Pre-Update Loss = 0.2158\n",
      "   Stopped Early after 12 steps (no improvement).\n",
      "   Post-Update Loss = 0.2155\n",
      "\n",
      "--- Outer Iteration 21/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: 1, improvement: 0.000023\n",
      "   Pre-Update Loss = 0.2155\n",
      "   Stopped Early after 11 steps (no improvement).\n",
      "   Post-Update Loss = 0.2154\n",
      "\n",
      "--- Outer Iteration 22/100 (Method 2) ---\n",
      "Random coordinate: 4, direction: 1, improvement: 0.000380\n",
      "   Pre-Update Loss = 0.2154\n",
      "   Post-Update Loss = 0.1983\n",
      "\n",
      "--- Outer Iteration 23/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000071\n",
      "   Pre-Update Loss = 0.1983\n",
      "   Post-Update Loss = 0.1961\n",
      "\n",
      "--- Outer Iteration 24/100 (Method 2) ---\n",
      "Random coordinate: 0, direction: -1, improvement: 0.000048\n",
      "   Pre-Update Loss = 0.1961\n",
      "   Stopped Early after 5 steps (no improvement).\n",
      "   Post-Update Loss = 0.1959\n",
      "\n",
      "--- Outer Iteration 25/100 (Method 2) ---\n",
      "Coordinate 0 yields no improvement in either direction. No update performed.\n",
      "\n",
      "--- Outer Iteration 26/100 (Method 2) ---\n",
      "Coordinate 0 yields no improvement in either direction. No update performed.\n",
      "\n",
      "--- Outer Iteration 27/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: 1, improvement: 0.000003\n",
      "   Pre-Update Loss = 0.1959\n",
      "   Stopped Early after 3 steps (no improvement).\n",
      "   Post-Update Loss = 0.1959\n",
      "\n",
      "--- Outer Iteration 28/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000477\n",
      "   Pre-Update Loss = 0.1959\n",
      "   Post-Update Loss = 0.1750\n",
      "\n",
      "--- Outer Iteration 29/100 (Method 2) ---\n",
      "Random coordinate: 2, direction: -1, improvement: 0.000064\n",
      "   Pre-Update Loss = 0.1750\n",
      "   Stopped Early after 18 steps (no improvement).\n",
      "   Post-Update Loss = 0.1744\n",
      "\n",
      "--- Outer Iteration 30/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: 1, improvement: 0.000033\n",
      "   Pre-Update Loss = 0.1744\n",
      "   Stopped Early after 35 steps (no improvement).\n",
      "   Post-Update Loss = 0.1738\n",
      "\n",
      "--- Outer Iteration 31/100 (Method 2) ---\n",
      "Random coordinate: 11, direction: 1, improvement: 0.000080\n",
      "   Pre-Update Loss = 0.1738\n",
      "   Stopped Early after 49 steps (no improvement).\n",
      "   Post-Update Loss = 0.1718\n",
      "\n",
      "--- Outer Iteration 32/100 (Method 2) ---\n",
      "Random coordinate: 8, direction: 1, improvement: 0.000056\n",
      "   Pre-Update Loss = 0.1718\n",
      "   Stopped Early after 25 steps (no improvement).\n",
      "   Post-Update Loss = 0.1711\n",
      "\n",
      "--- Outer Iteration 33/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: 1, improvement: 0.000005\n",
      "   Pre-Update Loss = 0.1711\n",
      "   Stopped Early after 4 steps (no improvement).\n",
      "   Post-Update Loss = 0.1711\n",
      "\n",
      "--- Outer Iteration 34/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000397\n",
      "   Pre-Update Loss = 0.1711\n",
      "   Post-Update Loss = 0.1537\n",
      "\n",
      "--- Outer Iteration 35/100 (Method 2) ---\n",
      "Random coordinate: 1, direction: -1, improvement: 0.000397\n",
      "   Pre-Update Loss = 0.1537\n",
      "   Post-Update Loss = 0.1357\n",
      "\n",
      "--- Outer Iteration 36/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: 1, improvement: 0.000028\n",
      "   Pre-Update Loss = 0.1357\n",
      "   Stopped Early after 17 steps (no improvement).\n",
      "   Post-Update Loss = 0.1355\n",
      "\n",
      "--- Outer Iteration 37/100 (Method 2) ---\n",
      "Random coordinate: 7, direction: 1, improvement: 0.000010\n",
      "   Pre-Update Loss = 0.1355\n",
      "   Stopped Early after 17 steps (no improvement).\n",
      "   Post-Update Loss = 0.1354\n",
      "\n",
      "--- Outer Iteration 38/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000025\n",
      "   Pre-Update Loss = 0.1354\n",
      "   Stopped Early after 27 steps (no improvement).\n",
      "   Post-Update Loss = 0.1350\n",
      "\n",
      "--- Outer Iteration 39/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: -1, improvement: 0.000001\n",
      "   Pre-Update Loss = 0.1350\n",
      "   Stopped Early after 1 steps (no improvement).\n",
      "   Post-Update Loss = 0.1350\n",
      "\n",
      "--- Outer Iteration 40/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000003\n",
      "   Pre-Update Loss = 0.1350\n",
      "   Stopped Early after 3 steps (no improvement).\n",
      "   Post-Update Loss = 0.1350\n",
      "\n",
      "--- Outer Iteration 41/100 (Method 2) ---\n",
      "Random coordinate: 0, direction: -1, improvement: 0.000039\n",
      "   Pre-Update Loss = 0.1350\n",
      "   Stopped Early after 5 steps (no improvement).\n",
      "   Post-Update Loss = 0.1349\n",
      "\n",
      "--- Outer Iteration 42/100 (Method 2) ---\n",
      "Coordinate 0 yields no improvement in either direction. No update performed.\n",
      "\n",
      "--- Outer Iteration 43/100 (Method 2) ---\n",
      "Random coordinate: 1, direction: -1, improvement: 0.000326\n",
      "   Pre-Update Loss = 0.1349\n",
      "   Post-Update Loss = 0.1202\n",
      "\n",
      "--- Outer Iteration 44/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: -1, improvement: 0.000008\n",
      "   Pre-Update Loss = 0.1202\n",
      "   Stopped Early after 6 steps (no improvement).\n",
      "   Post-Update Loss = 0.1202\n",
      "\n",
      "--- Outer Iteration 45/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: 1, improvement: 0.000047\n",
      "   Pre-Update Loss = 0.1202\n",
      "   Post-Update Loss = 0.1188\n",
      "\n",
      "--- Outer Iteration 46/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: 1, improvement: 0.000007\n",
      "   Pre-Update Loss = 0.1188\n",
      "   Stopped Early after 5 steps (no improvement).\n",
      "   Post-Update Loss = 0.1188\n",
      "\n",
      "--- Outer Iteration 47/100 (Method 2) ---\n",
      "Random coordinate: 2, direction: -1, improvement: 0.000061\n",
      "   Pre-Update Loss = 0.1188\n",
      "   Stopped Early after 22 steps (no improvement).\n",
      "   Post-Update Loss = 0.1181\n",
      "\n",
      "--- Outer Iteration 48/100 (Method 2) ---\n",
      "Random coordinate: 8, direction: 1, improvement: 0.000040\n",
      "   Pre-Update Loss = 0.1181\n",
      "   Stopped Early after 23 steps (no improvement).\n",
      "   Post-Update Loss = 0.1176\n",
      "\n",
      "--- Outer Iteration 49/100 (Method 2) ---\n",
      "Random coordinate: 7, direction: -1, improvement: 0.000011\n",
      "   Pre-Update Loss = 0.1176\n",
      "   Stopped Early after 19 steps (no improvement).\n",
      "   Post-Update Loss = 0.1175\n",
      "\n",
      "--- Outer Iteration 50/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000011\n",
      "   Pre-Update Loss = 0.1175\n",
      "   Stopped Early after 12 steps (no improvement).\n",
      "   Post-Update Loss = 0.1174\n",
      "\n",
      "--- Outer Iteration 51/100 (Method 2) ---\n",
      "Random coordinate: 4, direction: 1, improvement: 0.000234\n",
      "   Pre-Update Loss = 0.1174\n",
      "   Post-Update Loss = 0.1070\n",
      "\n",
      "--- Outer Iteration 52/100 (Method 2) ---\n",
      "Random coordinate: 9, direction: 1, improvement: 0.000040\n",
      "   Pre-Update Loss = 0.1070\n",
      "   Post-Update Loss = 0.1059\n",
      "\n",
      "--- Outer Iteration 53/100 (Method 2) ---\n",
      "Random coordinate: 8, direction: -1, improvement: 0.000021\n",
      "   Pre-Update Loss = 0.1059\n",
      "   Stopped Early after 13 steps (no improvement).\n",
      "   Post-Update Loss = 0.1058\n",
      "\n",
      "--- Outer Iteration 54/100 (Method 2) ---\n",
      "Random coordinate: 4, direction: 1, improvement: 0.000183\n",
      "   Pre-Update Loss = 0.1058\n",
      "   Post-Update Loss = 0.0979\n",
      "\n",
      "--- Outer Iteration 55/100 (Method 2) ---\n",
      "Random coordinate: 0, direction: 1, improvement: 0.000010\n",
      "   Pre-Update Loss = 0.0979\n",
      "   Stopped Early after 2 steps (no improvement).\n",
      "   Post-Update Loss = 0.0979\n",
      "\n",
      "--- Outer Iteration 56/100 (Method 2) ---\n",
      "Coordinate 0 yields no improvement in either direction. No update performed.\n",
      "\n",
      "--- Outer Iteration 57/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000129\n",
      "   Pre-Update Loss = 0.0979\n",
      "   Post-Update Loss = 0.0927\n",
      "\n",
      "--- Outer Iteration 58/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000032\n",
      "   Pre-Update Loss = 0.0927\n",
      "   Stopped Early after 32 steps (no improvement).\n",
      "   Post-Update Loss = 0.0922\n",
      "\n",
      "--- Outer Iteration 59/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000087\n",
      "   Pre-Update Loss = 0.0922\n",
      "   Post-Update Loss = 0.0890\n",
      "\n",
      "--- Outer Iteration 60/100 (Method 2) ---\n",
      "Coordinate 5 yields no improvement in either direction. No update performed.\n",
      "\n",
      "--- Outer Iteration 61/100 (Method 2) ---\n",
      "Random coordinate: 4, direction: 1, improvement: 0.000135\n",
      "   Pre-Update Loss = 0.0890\n",
      "   Post-Update Loss = 0.0833\n",
      "\n",
      "--- Outer Iteration 62/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000045\n",
      "   Pre-Update Loss = 0.0833\n",
      "   Post-Update Loss = 0.0819\n",
      "\n",
      "--- Outer Iteration 63/100 (Method 2) ---\n",
      "Random coordinate: 1, direction: -1, improvement: 0.000173\n",
      "   Pre-Update Loss = 0.0819\n",
      "   Post-Update Loss = 0.0741\n",
      "\n",
      "--- Outer Iteration 64/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: 1, improvement: 0.000012\n",
      "   Pre-Update Loss = 0.0741\n",
      "   Stopped Early after 26 steps (no improvement).\n",
      "   Post-Update Loss = 0.0740\n",
      "\n",
      "--- Outer Iteration 65/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: 1, improvement: 0.000002\n",
      "   Pre-Update Loss = 0.0740\n",
      "   Stopped Early after 3 steps (no improvement).\n",
      "   Post-Update Loss = 0.0740\n",
      "\n",
      "--- Outer Iteration 66/100 (Method 2) ---\n",
      "Coordinate 6 yields no improvement in either direction. No update performed.\n",
      "\n",
      "--- Outer Iteration 67/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000039\n",
      "   Pre-Update Loss = 0.0740\n",
      "   Post-Update Loss = 0.0730\n",
      "\n",
      "--- Outer Iteration 68/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000014\n",
      "   Pre-Update Loss = 0.0730\n",
      "   Stopped Early after 24 steps (no improvement).\n",
      "   Post-Update Loss = 0.0728\n",
      "\n",
      "--- Outer Iteration 69/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000125\n",
      "   Pre-Update Loss = 0.0728\n",
      "   Post-Update Loss = 0.0676\n",
      "\n",
      "--- Outer Iteration 70/100 (Method 2) ---\n",
      "Random coordinate: 0, direction: -1, improvement: 0.000174\n",
      "   Pre-Update Loss = 0.0676\n",
      "   Stopped Early after 41 steps (no improvement).\n",
      "   Post-Update Loss = 0.0640\n",
      "\n",
      "--- Outer Iteration 71/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000129\n",
      "   Pre-Update Loss = 0.0640\n",
      "   Post-Update Loss = 0.0586\n",
      "\n",
      "--- Outer Iteration 72/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000008\n",
      "   Pre-Update Loss = 0.0586\n",
      "   Stopped Early after 17 steps (no improvement).\n",
      "   Post-Update Loss = 0.0586\n",
      "\n",
      "--- Outer Iteration 73/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: 1, improvement: 0.000020\n",
      "   Pre-Update Loss = 0.0586\n",
      "   Post-Update Loss = 0.0580\n",
      "\n",
      "--- Outer Iteration 74/100 (Method 2) ---\n",
      "Random coordinate: 11, direction: 1, improvement: 0.000036\n",
      "   Pre-Update Loss = 0.0580\n",
      "   Post-Update Loss = 0.0570\n",
      "\n",
      "--- Outer Iteration 75/100 (Method 2) ---\n",
      "Random coordinate: 0, direction: -1, improvement: 0.000041\n",
      "   Pre-Update Loss = 0.0570\n",
      "   Stopped Early after 11 steps (no improvement).\n",
      "   Post-Update Loss = 0.0568\n",
      "\n",
      "--- Outer Iteration 76/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000109\n",
      "   Pre-Update Loss = 0.0568\n",
      "   Post-Update Loss = 0.0522\n",
      "\n",
      "--- Outer Iteration 77/100 (Method 2) ---\n",
      "Random coordinate: 9, direction: 1, improvement: 0.000018\n",
      "   Pre-Update Loss = 0.0522\n",
      "   Post-Update Loss = 0.0518\n",
      "\n",
      "--- Outer Iteration 78/100 (Method 2) ---\n",
      "Random coordinate: 8, direction: -1, improvement: 0.000018\n",
      "   Pre-Update Loss = 0.0518\n",
      "   Stopped Early after 24 steps (no improvement).\n",
      "   Post-Update Loss = 0.0515\n",
      "\n",
      "--- Outer Iteration 79/100 (Method 2) ---\n",
      "Random coordinate: 7, direction: 1, improvement: 0.000007\n",
      "   Pre-Update Loss = 0.0515\n",
      "   Stopped Early after 19 steps (no improvement).\n",
      "   Post-Update Loss = 0.0515\n",
      "\n",
      "--- Outer Iteration 80/100 (Method 2) ---\n",
      "Random coordinate: 1, direction: -1, improvement: 0.000099\n",
      "   Pre-Update Loss = 0.0515\n",
      "   Post-Update Loss = 0.0471\n",
      "\n",
      "--- Outer Iteration 81/100 (Method 2) ---\n",
      "Random coordinate: 11, direction: 1, improvement: 0.000018\n",
      "   Pre-Update Loss = 0.0471\n",
      "   Stopped Early after 34 steps (no improvement).\n",
      "   Post-Update Loss = 0.0467\n",
      "\n",
      "--- Outer Iteration 82/100 (Method 2) ---\n",
      "Random coordinate: 5, direction: -1, improvement: 0.000029\n",
      "   Pre-Update Loss = 0.0467\n",
      "   Stopped Early after 44 steps (no improvement).\n",
      "   Post-Update Loss = 0.0461\n",
      "\n",
      "--- Outer Iteration 83/100 (Method 2) ---\n",
      "Random coordinate: 7, direction: 1, improvement: 0.000001\n",
      "   Pre-Update Loss = 0.0461\n",
      "   Stopped Early after 3 steps (no improvement).\n",
      "   Post-Update Loss = 0.0461\n",
      "\n",
      "--- Outer Iteration 84/100 (Method 2) ---\n",
      "Random coordinate: 1, direction: -1, improvement: 0.000081\n",
      "   Pre-Update Loss = 0.0461\n",
      "   Post-Update Loss = 0.0425\n",
      "\n",
      "--- Outer Iteration 85/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000022\n",
      "   Pre-Update Loss = 0.0425\n",
      "   Stopped Early after 42 steps (no improvement).\n",
      "   Post-Update Loss = 0.0420\n",
      "\n",
      "--- Outer Iteration 86/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000011\n",
      "   Pre-Update Loss = 0.0420\n",
      "   Stopped Early after 28 steps (no improvement).\n",
      "   Post-Update Loss = 0.0419\n",
      "\n",
      "--- Outer Iteration 87/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000043\n",
      "   Pre-Update Loss = 0.0419\n",
      "   Post-Update Loss = 0.0405\n",
      "\n",
      "--- Outer Iteration 88/100 (Method 2) ---\n",
      "Random coordinate: 9, direction: 1, improvement: 0.000005\n",
      "   Pre-Update Loss = 0.0405\n",
      "   Stopped Early after 16 steps (no improvement).\n",
      "   Post-Update Loss = 0.0404\n",
      "\n",
      "--- Outer Iteration 89/100 (Method 2) ---\n",
      "Random coordinate: 2, direction: -1, improvement: 0.000034\n",
      "   Pre-Update Loss = 0.0404\n",
      "   Stopped Early after 32 steps (no improvement).\n",
      "   Post-Update Loss = 0.0399\n",
      "\n",
      "--- Outer Iteration 90/100 (Method 2) ---\n",
      "Random coordinate: 6, direction: 1, improvement: 0.000008\n",
      "   Pre-Update Loss = 0.0399\n",
      "   Stopped Early after 29 steps (no improvement).\n",
      "   Post-Update Loss = 0.0398\n",
      "\n",
      "--- Outer Iteration 91/100 (Method 2) ---\n",
      "Random coordinate: 4, direction: 1, improvement: 0.000066\n",
      "   Pre-Update Loss = 0.0398\n",
      "   Post-Update Loss = 0.0370\n",
      "\n",
      "--- Outer Iteration 92/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000050\n",
      "   Pre-Update Loss = 0.0370\n",
      "   Post-Update Loss = 0.0351\n",
      "\n",
      "--- Outer Iteration 93/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: 1, improvement: 0.000013\n",
      "   Pre-Update Loss = 0.0351\n",
      "   Stopped Early after 42 steps (no improvement).\n",
      "   Post-Update Loss = 0.0348\n",
      "\n",
      "--- Outer Iteration 94/100 (Method 2) ---\n",
      "Random coordinate: 3, direction: -1, improvement: 0.000041\n",
      "   Pre-Update Loss = 0.0348\n",
      "   Post-Update Loss = 0.0334\n",
      "\n",
      "--- Outer Iteration 95/100 (Method 2) ---\n",
      "Random coordinate: 12, direction: -1, improvement: 0.000008\n",
      "   Pre-Update Loss = 0.0334\n",
      "   Stopped Early after 19 steps (no improvement).\n",
      "   Post-Update Loss = 0.0333\n",
      "\n",
      "--- Outer Iteration 96/100 (Method 2) ---\n",
      "Random coordinate: 0, direction: -1, improvement: 0.000041\n",
      "   Pre-Update Loss = 0.0333\n",
      "   Stopped Early after 16 steps (no improvement).\n",
      "   Post-Update Loss = 0.0330\n",
      "\n",
      "--- Outer Iteration 97/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: -1, improvement: 0.000017\n",
      "   Pre-Update Loss = 0.0330\n",
      "   Post-Update Loss = 0.0326\n",
      "\n",
      "--- Outer Iteration 98/100 (Method 2) ---\n",
      "Random coordinate: 2, direction: -1, improvement: 0.000014\n",
      "   Pre-Update Loss = 0.0326\n",
      "   Stopped Early after 17 steps (no improvement).\n",
      "   Post-Update Loss = 0.0324\n",
      "\n",
      "--- Outer Iteration 99/100 (Method 2) ---\n",
      "Random coordinate: 13, direction: -1, improvement: 0.000043\n",
      "   Pre-Update Loss = 0.0324\n",
      "   Post-Update Loss = 0.0308\n",
      "\n",
      "--- Outer Iteration 100/100 (Method 2) ---\n",
      "Random coordinate: 10, direction: 1, improvement: 0.000012\n",
      "   Pre-Update Loss = 0.0308\n",
      "   Stopped Early after 45 steps (no improvement).\n",
      "   Post-Update Loss = 0.0305\n",
      "\n",
      "Final weights (Method 2): [-0.28976086 -3.31537323 -0.52736894 -3.21243847  2.83232851 -0.65239453\n",
      "  0.737598   -0.93122197  0.62798883  1.22609272 -2.35239507  1.5594831\n",
      " -2.57894639 -4.9095673 ]\n",
      "Final Loss (Method 2): 0.0305\n",
      "\n",
      "=== Logistic Regression (No Regularization, Warm Start) ===\n",
      "Iteration 1/100 Logistic Regression Loss: 0.0977\n",
      "Iteration 2/100 Logistic Regression Loss: 0.0674\n",
      "Iteration 3/100 Logistic Regression Loss: 0.0531\n",
      "Iteration 4/100 Logistic Regression Loss: 0.0445\n",
      "Iteration 5/100 Logistic Regression Loss: 0.0386\n",
      "Iteration 6/100 Logistic Regression Loss: 0.0343\n",
      "Iteration 7/100 Logistic Regression Loss: 0.0310\n",
      "Iteration 8/100 Logistic Regression Loss: 0.0284\n",
      "Iteration 9/100 Logistic Regression Loss: 0.0263\n",
      "Iteration 10/100 Logistic Regression Loss: 0.0245\n",
      "Iteration 11/100 Logistic Regression Loss: 0.0230\n",
      "Iteration 12/100 Logistic Regression Loss: 0.0216\n",
      "Iteration 13/100 Logistic Regression Loss: 0.0205\n",
      "Iteration 14/100 Logistic Regression Loss: 0.0195\n",
      "Iteration 15/100 Logistic Regression Loss: 0.0186\n",
      "Iteration 16/100 Logistic Regression Loss: 0.0178\n",
      "Iteration 17/100 Logistic Regression Loss: 0.0171\n",
      "Iteration 18/100 Logistic Regression Loss: 0.0164\n",
      "Iteration 19/100 Logistic Regression Loss: 0.0158\n",
      "Iteration 20/100 Logistic Regression Loss: 0.0152\n",
      "Iteration 21/100 Logistic Regression Loss: 0.0147\n",
      "Iteration 22/100 Logistic Regression Loss: 0.0143\n",
      "Iteration 23/100 Logistic Regression Loss: 0.0138\n",
      "Iteration 24/100 Logistic Regression Loss: 0.0134\n",
      "Iteration 25/100 Logistic Regression Loss: 0.0131\n",
      "Iteration 26/100 Logistic Regression Loss: 0.0127\n",
      "Iteration 27/100 Logistic Regression Loss: 0.0124\n",
      "Iteration 28/100 Logistic Regression Loss: 0.0121\n",
      "Iteration 29/100 Logistic Regression Loss: 0.0118\n",
      "Iteration 30/100 Logistic Regression Loss: 0.0115\n",
      "Iteration 31/100 Logistic Regression Loss: 0.0112\n",
      "Iteration 32/100 Logistic Regression Loss: 0.0110\n",
      "Iteration 33/100 Logistic Regression Loss: 0.0108\n",
      "Iteration 34/100 Logistic Regression Loss: 0.0105\n",
      "Iteration 35/100 Logistic Regression Loss: 0.0103\n",
      "Iteration 36/100 Logistic Regression Loss: 0.0101\n",
      "Iteration 37/100 Logistic Regression Loss: 0.0099\n",
      "Iteration 38/100 Logistic Regression Loss: 0.0098\n",
      "Iteration 39/100 Logistic Regression Loss: 0.0096\n",
      "Iteration 40/100 Logistic Regression Loss: 0.0094\n",
      "Iteration 41/100 Logistic Regression Loss: 0.0092\n",
      "Iteration 42/100 Logistic Regression Loss: 0.0091\n",
      "Iteration 43/100 Logistic Regression Loss: 0.0089\n",
      "Iteration 44/100 Logistic Regression Loss: 0.0088\n",
      "Iteration 45/100 Logistic Regression Loss: 0.0087\n",
      "Iteration 46/100 Logistic Regression Loss: 0.0085\n",
      "Iteration 47/100 Logistic Regression Loss: 0.0084\n",
      "Iteration 48/100 Logistic Regression Loss: 0.0083\n",
      "Iteration 49/100 Logistic Regression Loss: 0.0082\n",
      "Iteration 50/100 Logistic Regression Loss: 0.0080\n",
      "Iteration 51/100 Logistic Regression Loss: 0.0079\n",
      "Iteration 52/100 Logistic Regression Loss: 0.0078\n",
      "Iteration 53/100 Logistic Regression Loss: 0.0077\n",
      "Iteration 54/100 Logistic Regression Loss: 0.0076\n",
      "Iteration 55/100 Logistic Regression Loss: 0.0075\n",
      "Iteration 56/100 Logistic Regression Loss: 0.0074\n",
      "Iteration 57/100 Logistic Regression Loss: 0.0073\n",
      "Iteration 58/100 Logistic Regression Loss: 0.0072\n",
      "Iteration 59/100 Logistic Regression Loss: 0.0072\n",
      "Iteration 60/100 Logistic Regression Loss: 0.0071\n",
      "Iteration 61/100 Logistic Regression Loss: 0.0070\n",
      "Iteration 62/100 Logistic Regression Loss: 0.0069\n",
      "Iteration 63/100 Logistic Regression Loss: 0.0068\n",
      "Iteration 64/100 Logistic Regression Loss: 0.0068\n",
      "Iteration 65/100 Logistic Regression Loss: 0.0067\n",
      "Iteration 66/100 Logistic Regression Loss: 0.0066\n",
      "Iteration 67/100 Logistic Regression Loss: 0.0065\n",
      "Iteration 68/100 Logistic Regression Loss: 0.0065\n",
      "Iteration 69/100 Logistic Regression Loss: 0.0064\n",
      "Iteration 70/100 Logistic Regression Loss: 0.0063\n",
      "Iteration 71/100 Logistic Regression Loss: 0.0063\n",
      "Iteration 72/100 Logistic Regression Loss: 0.0062\n",
      "Iteration 73/100 Logistic Regression Loss: 0.0062\n",
      "Iteration 74/100 Logistic Regression Loss: 0.0061\n",
      "Iteration 75/100 Logistic Regression Loss: 0.0060\n",
      "Iteration 76/100 Logistic Regression Loss: 0.0060\n",
      "Iteration 77/100 Logistic Regression Loss: 0.0059\n",
      "Iteration 78/100 Logistic Regression Loss: 0.0059\n",
      "Iteration 79/100 Logistic Regression Loss: 0.0058\n",
      "Iteration 80/100 Logistic Regression Loss: 0.0058\n",
      "Iteration 81/100 Logistic Regression Loss: 0.0057\n",
      "Iteration 82/100 Logistic Regression Loss: 0.0057\n",
      "Iteration 83/100 Logistic Regression Loss: 0.0056\n",
      "Iteration 84/100 Logistic Regression Loss: 0.0056\n",
      "Iteration 85/100 Logistic Regression Loss: 0.0055\n",
      "Iteration 86/100 Logistic Regression Loss: 0.0055\n",
      "Iteration 87/100 Logistic Regression Loss: 0.0054\n",
      "Iteration 88/100 Logistic Regression Loss: 0.0054\n",
      "Iteration 89/100 Logistic Regression Loss: 0.0054\n",
      "Iteration 90/100 Logistic Regression Loss: 0.0053\n",
      "Iteration 91/100 Logistic Regression Loss: 0.0053\n",
      "Iteration 92/100 Logistic Regression Loss: 0.0052\n",
      "Iteration 93/100 Logistic Regression Loss: 0.0052\n",
      "Iteration 94/100 Logistic Regression Loss: 0.0052\n",
      "Iteration 95/100 Logistic Regression Loss: 0.0051\n",
      "Iteration 96/100 Logistic Regression Loss: 0.0051\n",
      "Iteration 97/100 Logistic Regression Loss: 0.0050\n",
      "Iteration 98/100 Logistic Regression Loss: 0.0050\n",
      "Iteration 99/100 Logistic Regression Loss: 0.0050\n",
      "Iteration 100/100 Logistic Regression Loss: 0.0049\n",
      "\n",
      "Final Loss from Logistic Regression: 0.0049\n",
      "\n",
      "\n",
      "=====================Final Losses=====================\n",
      "Final Loss (Method 1): 0.0036\n",
      "Final Loss (Method 2): 0.0305\n",
      "Final Loss from Logistic Regression: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and preprocess data\n",
    "# ---------------------------\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "# Restrict to classes 0 and 1\n",
    "df = df[df['target'] < 2]\n",
    "\n",
    "X = df[wine.feature_names].values  # Shape: (N, d)\n",
    "y = df['target'].values            # Shape: (N,)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Normalize each feature to be in [-1, 1]\n",
    "# ---------------------------\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_range = X_max - X_min\n",
    "X_range[X_range == 0] = 1e-9  # safeguard against division by zero\n",
    "X_norm = 2.0 * (X - X_min) / X_range - 1.0\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Initialize weights via linear regression\n",
    "# ---------------------------\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_norm, y)\n",
    "# w_init: intercept + coefficients\n",
    "w_init = np.concatenate(([linreg.intercept_], linreg.coef_))\n",
    "X_aug = np.hstack([np.ones((X_norm.shape[0], 1)), X_norm])\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Helper functions\n",
    "# ---------------------------\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(X_aug, y, w):\n",
    "    eps = 1e-10\n",
    "    preds = sigmoid(X_aug @ w)\n",
    "    return -np.mean(y * np.log(preds + eps) + (1 - y) * np.log(1 - preds + eps))\n",
    "\n",
    "def find_best_coordinate_and_direction(X_aug, y, w, step_size):\n",
    "    current_loss = logistic_loss(X_aug, y, w)\n",
    "    best_coord, best_direction, best_improvement = 0, 0, 0.0\n",
    "    for i in range(len(w)):\n",
    "        w_temp_plus = w.copy()\n",
    "        w_temp_plus[i] += step_size\n",
    "        plus_loss = logistic_loss(X_aug, y, w_temp_plus)\n",
    "        improvement_plus = current_loss - plus_loss\n",
    "\n",
    "        w_temp_minus = w.copy()\n",
    "        w_temp_minus[i] -= step_size\n",
    "        minus_loss = logistic_loss(X_aug, y, w_temp_minus)\n",
    "        improvement_minus = current_loss - minus_loss\n",
    "\n",
    "        if improvement_plus > improvement_minus:\n",
    "            improvement_for_i = improvement_plus\n",
    "            direction_for_i = +1\n",
    "        else:\n",
    "            improvement_for_i = improvement_minus\n",
    "            direction_for_i = -1\n",
    "\n",
    "        if improvement_for_i > best_improvement:\n",
    "            best_improvement = improvement_for_i\n",
    "            best_coord = i\n",
    "            best_direction = direction_for_i\n",
    "    return best_coord, best_direction, best_improvement\n",
    "\n",
    "def pick_random_coordinate_and_direction(X_aug, y, w, step_size):\n",
    "    current_loss = logistic_loss(X_aug, y, w)\n",
    "    coord = np.random.randint(0, len(w))\n",
    "    \n",
    "    w_temp_plus = w.copy()\n",
    "    w_temp_plus[coord] += step_size\n",
    "    plus_loss = logistic_loss(X_aug, y, w_temp_plus)\n",
    "    improvement_plus = current_loss - plus_loss\n",
    "\n",
    "    w_temp_minus = w.copy()\n",
    "    w_temp_minus[coord] -= step_size\n",
    "    minus_loss = logistic_loss(X_aug, y, w_temp_minus)\n",
    "    improvement_minus = current_loss - minus_loss\n",
    "\n",
    "    if improvement_plus > improvement_minus:\n",
    "        best_direction = +1\n",
    "        best_improvement = improvement_plus\n",
    "    else:\n",
    "        best_direction = -1\n",
    "        best_improvement = improvement_minus\n",
    "    return coord, best_direction, best_improvement\n",
    "\n",
    "def coordinate_descent_update(X_aug, y, w, coord, direction, step_size, num_steps=5):\n",
    "    curr_loss = logistic_loss(X_aug, y, w)\n",
    "    print(f\"   Pre-Update Loss = {curr_loss:.4f}\")\n",
    "    for step in range(num_steps):\n",
    "        w[coord] += direction * step_size\n",
    "        new_loss = logistic_loss(X_aug, y, w)\n",
    "        if new_loss > curr_loss:\n",
    "            w[coord] -= direction * step_size\n",
    "            print(f\"   Stopped Early after {step} steps (no improvement).\")\n",
    "            break\n",
    "        else:\n",
    "            curr_loss = new_loss\n",
    "    print(f\"   Post-Update Loss = {logistic_loss(X_aug, y, w):.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Fixed parameters\n",
    "# ---------------------------\n",
    "num_outer_iters = 100\n",
    "step_size = 0.01\n",
    "num_steps = 50\n",
    "\n",
    "# Record histories\n",
    "method1_loss_history = []\n",
    "method2_loss_history = []\n",
    "logreg_loss_history = []\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Method 1: Best Coordinate Descent\n",
    "# ---------------------------\n",
    "print(\"\\n=== Method 1: Best Coordinate Descent ===\")\n",
    "w_method1 = w_init.copy()\n",
    "initial_loss_method1 = logistic_loss(X_aug, y, w_method1)\n",
    "print(f\"Initial Loss (Method 1): {initial_loss_method1:.4f}\")\n",
    "method1_loss_history.append(initial_loss_method1)\n",
    "\n",
    "for iteration in range(num_outer_iters):\n",
    "    print(f\"\\n--- Outer Iteration {iteration+1}/{num_outer_iters} (Method 1) ---\")\n",
    "    best_coord, best_direction, best_improvement = find_best_coordinate_and_direction(X_aug, y, w_method1, step_size)\n",
    "    if best_improvement <= 0:\n",
    "        print(\"No further improvement found. Stopping early.\")\n",
    "        break\n",
    "    print(f\"Best coordinate: {best_coord}, direction: {best_direction}, improvement: {best_improvement:.4f}\")\n",
    "    coordinate_descent_update(X_aug, y, w_method1, best_coord, best_direction, step_size, num_steps=num_steps)\n",
    "    current_loss = logistic_loss(X_aug, y, w_method1)\n",
    "    method1_loss_history.append(current_loss)\n",
    "final_loss_method1 = logistic_loss(X_aug, y, w_method1)\n",
    "print(\"\\nFinal weights (Method 1):\", w_method1)\n",
    "print(f\"Final Loss (Method 1): {final_loss_method1:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Method 2: Random Coordinate Descent\n",
    "# ---------------------------\n",
    "print(\"\\n=== Method 2: Random Coordinate Descent ===\")\n",
    "w_method2 = w_init.copy()\n",
    "initial_loss_method2 = logistic_loss(X_aug, y, w_method2)\n",
    "print(f\"Initial Loss (Method 2): {initial_loss_method2:.4f}\")\n",
    "method2_loss_history.append(initial_loss_method2)\n",
    "\n",
    "for iteration in range(num_outer_iters):\n",
    "    print(f\"\\n--- Outer Iteration {iteration+1}/{num_outer_iters} (Method 2) ---\")\n",
    "    coord, direction, improvement = pick_random_coordinate_and_direction(X_aug, y, w_method2, step_size)\n",
    "    if improvement <= 0:\n",
    "        print(f\"Coordinate {coord} yields no improvement in either direction. No update performed.\")\n",
    "    else:\n",
    "        print(f\"Random coordinate: {coord}, direction: {direction}, improvement: {improvement:.6f}\")\n",
    "        coordinate_descent_update(X_aug, y, w_method2, coord, direction, step_size, num_steps=num_steps)\n",
    "    current_loss = logistic_loss(X_aug, y, w_method2)\n",
    "    method2_loss_history.append(current_loss)\n",
    "final_loss_method2 = logistic_loss(X_aug, y, w_method2)\n",
    "print(\"\\nFinal weights (Method 2):\", w_method2)\n",
    "print(f\"Final Loss (Method 2): {final_loss_method2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Logistic Regression (Warm Start) for same # of iters\n",
    "# ---------------------------\n",
    "print(\"\\n=== Logistic Regression (No Regularization, Warm Start) ===\")\n",
    "logreg = LogisticRegression(penalty=None, solver='saga', warm_start=True, max_iter=1, random_state=42)\n",
    "for iteration in range(num_outer_iters):\n",
    "    logreg.fit(X_norm, y)\n",
    "    w_logreg = np.concatenate(([logreg.intercept_[0]], logreg.coef_[0]))\n",
    "    current_logreg_loss = logistic_loss(X_aug, y, w_logreg)\n",
    "    logreg_loss_history.append(current_logreg_loss)\n",
    "    print(f\"Iteration {iteration+1}/{num_outer_iters} Logistic Regression Loss: {current_logreg_loss:.4f}\")\n",
    "final_logreg_loss = logreg_loss_history[-1]\n",
    "print(f\"\\nFinal Loss from Logistic Regression: {final_logreg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\\n=====================Final Losses=====================\")\n",
    "print(f\"Final Loss (Method 1): {final_loss_method1:.4f}\")\n",
    "print(f\"Final Loss (Method 2): {final_loss_method2:.4f}\")\n",
    "print(f\"Final Loss from Logistic Regression: {final_logreg_loss:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Plot Loss History Comparison\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create labels that contain the final losses:\n",
    "m1_label = f\"Method 1 (Best Coord) - Final={final_loss_method1:.4f}\"\n",
    "m2_label = f\"Method 2 (Random Coord) - Final={final_loss_method2:.4f}\"\n",
    "lr_label = f\"Logistic Regression - Final={final_logreg_loss:.4f}\"\n",
    "\n",
    "plt.plot(method1_loss_history, label=m1_label, marker='o')\n",
    "plt.plot(method2_loss_history, label=m2_label, marker='s')\n",
    "plt.plot(logreg_loss_history, label=lr_label, marker='^')\n",
    "\n",
    "plt.xlabel('Outer Iteration')\n",
    "plt.ylabel('Logistic Loss')\n",
    "plt.title('Loss History Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-sparse comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\begin{tabular}{c|cc}\n",
      "\\hline\n",
      " $k$ & CD k-sparse Loss & SelectKBest+LR Loss \\\\\n",
      "\\hline\n",
      "1 & 0.2782 & 0.1585 \\\\\n",
      "2 & 0.0967 & 0.0758 \\\\\n",
      "3 & 0.0758 & 0.0394 \\\\\n",
      "5 & 0.0376 & 0.0001 \\\\\n",
      "7 & 0.0054 & 0.0004 \\\\\n",
      "10 & 0.0040 & 0.0007 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Comparison of final logistic losses for various $k$.}\n",
      "\\label{tab:k_sparse_comparison}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ===========================\n",
    "# 1. Load and preprocess data\n",
    "# ===========================\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "df = df[df['target'] < 2]  # Restrict to binary classes 0 and 1\n",
    "\n",
    "X = df[wine.feature_names].values\n",
    "y = df['target'].values\n",
    "\n",
    "# ===========================\n",
    "# 2. Normalize to [-1, 1]\n",
    "# ===========================\n",
    "X_min = X.min(axis=0)\n",
    "X_max = X.max(axis=0)\n",
    "X_range = X_max - X_min\n",
    "X_range[X_range == 0] = 1e-9  # Avoid division by zero\n",
    "X_norm = 2.0 * (X - X_min) / X_range - 1.0\n",
    "\n",
    "# For our custom coordinate descent approach, we augment X with a column of 1s for the intercept\n",
    "n_samples, n_features = X_norm.shape\n",
    "X_aug = np.hstack([np.ones((n_samples, 1)), X_norm])  # shape: (N, d+1)\n",
    "\n",
    "# ===========================\n",
    "# 3. Helper functions\n",
    "# ===========================\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(X_aug, y, w):\n",
    "    eps = 1e-10\n",
    "    preds = sigmoid(X_aug @ w)\n",
    "    return -np.mean(y * np.log(preds + eps) + (1 - y) * np.log(1 - preds + eps))\n",
    "\n",
    "def find_best_coordinate_and_direction_k_sparse(X_aug, y, w, step_size, active_dims, k):\n",
    "    \"\"\"\n",
    "    - If len(active_dims) < k, search among all coordinates.\n",
    "    - Otherwise, only search among active_dims.\n",
    "    Returns: (best_coord, best_direction, best_improvement).\n",
    "    Possibly updates active_dims if we discover a new coordinate and we haven't hit k yet.\n",
    "    \"\"\"\n",
    "    current_loss = logistic_loss(X_aug, y, w)\n",
    "    best_coord = None\n",
    "    best_direction = 0\n",
    "    best_improvement = 0.0\n",
    "    \n",
    "    # Coordinates to search\n",
    "    if len(active_dims) < k:\n",
    "        search_coords = range(len(w))\n",
    "    else:\n",
    "        search_coords = active_dims\n",
    "\n",
    "    for i in search_coords:\n",
    "        # Try + step\n",
    "        w_temp_plus = w.copy()\n",
    "        w_temp_plus[i] += step_size\n",
    "        plus_loss = logistic_loss(X_aug, y, w_temp_plus)\n",
    "        improvement_plus = current_loss - plus_loss\n",
    "\n",
    "        # Try - step\n",
    "        w_temp_minus = w.copy()\n",
    "        w_temp_minus[i] -= step_size\n",
    "        minus_loss = logistic_loss(X_aug, y, w_temp_minus)\n",
    "        improvement_minus = current_loss - minus_loss\n",
    "\n",
    "        if improvement_plus > improvement_minus:\n",
    "            improvement_for_i = improvement_plus\n",
    "            direction_for_i = +1\n",
    "        else:\n",
    "            improvement_for_i = improvement_minus\n",
    "            direction_for_i = -1\n",
    "\n",
    "        if improvement_for_i > best_improvement:\n",
    "            best_improvement = improvement_for_i\n",
    "            best_coord = i\n",
    "            best_direction = direction_for_i\n",
    "\n",
    "    # If we found a coordinate that improves and haven't reached k active dims yet\n",
    "    if best_coord is not None and len(active_dims) < k:\n",
    "        if best_coord not in active_dims:\n",
    "            active_dims.add(best_coord)\n",
    "\n",
    "    return best_coord, best_direction, best_improvement\n",
    "\n",
    "def coordinate_descent_update(X_aug, y, w, coord, direction, step_size, num_steps=5):\n",
    "    curr_loss = logistic_loss(X_aug, y, w)\n",
    "    for _ in range(num_steps):\n",
    "        w[coord] += direction * step_size\n",
    "        new_loss = logistic_loss(X_aug, y, w)\n",
    "        if new_loss > curr_loss:\n",
    "            w[coord] -= direction * step_size  # revert\n",
    "            break\n",
    "        else:\n",
    "            curr_loss = new_loss\n",
    "\n",
    "def run_k_sparse_cd(X_aug, y, k, max_iters=100, step_size=0.01, num_steps=50):\n",
    "    \"\"\"\n",
    "    Runs coordinate descent with a maximum of k active coordinates.\n",
    "    w is initialized to zeros (including intercept).\n",
    "    Returns final loss and the final weight vector.\n",
    "    \"\"\"\n",
    "    d_plus_1 = X_aug.shape[1]\n",
    "    w = np.zeros(d_plus_1, dtype=float)  # intercept + features = all zeros\n",
    "    active_dims = set()\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        best_coord, best_direction, best_improvement = find_best_coordinate_and_direction_k_sparse(\n",
    "            X_aug, y, w, step_size, active_dims, k\n",
    "        )\n",
    "        if best_coord is None or best_improvement <= 0:\n",
    "            break\n",
    "        coordinate_descent_update(X_aug, y, w, best_coord, best_direction, step_size, num_steps)\n",
    "    \n",
    "    final_loss = logistic_loss(X_aug, y, w)\n",
    "    return final_loss, w, active_dims\n",
    "\n",
    "# ===========================\n",
    "# 4. Compare for a list of k\n",
    "# ===========================\n",
    "k_list = [1, 2, 3, 5, 7, 10]\n",
    "\n",
    "cd_losses = []   # coordinate descent k-sparse\n",
    "sk_losses = []   # SelectKBest + LR pipeline\n",
    "\n",
    "for k in k_list:\n",
    "    # 4A) k-sparse coordinate descent\n",
    "    loss_cd, w_cd, active_dims = run_k_sparse_cd(\n",
    "        X_aug, y, k, max_iters=100, step_size=0.01, num_steps=50\n",
    "    )\n",
    "    cd_losses.append(loss_cd)\n",
    "    \n",
    "    # 4B) \"SelectKBest + LR(penalty='none')\"\n",
    "    pipe = Pipeline([\n",
    "        (\"select\", SelectKBest(score_func=f_classif, k=k)),\n",
    "        (\"logreg\", LogisticRegression(penalty=None, max_iter=1000))\n",
    "    ])\n",
    "    pipe.fit(X_norm, y)\n",
    "    \n",
    "    # Evaluate final logistic loss\n",
    "    preds = pipe.predict_proba(X_norm)[:, 1]\n",
    "    eps = 1e-10\n",
    "    loss_sk = -np.mean(y * np.log(preds + eps) + (1 - y) * np.log(1 - preds + eps))\n",
    "    sk_losses.append(loss_sk)\n",
    "\n",
    "# ===========================\n",
    "# 5. Print a Table of Results\n",
    "# ===========================\n",
    "print(\"\\\\begin{table}[h]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\begin{tabular}{c|cc}\")\n",
    "print(\"\\\\hline\")\n",
    "print(\" $k$ & CD k-sparse Loss & SelectKBest+LR Loss \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "for i, k_val in enumerate(k_list):\n",
    "    print(f\"{k_val} & {cd_losses[i]:.4f} & {sk_losses[i]:.4f} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\caption{Comparison of final logistic losses for various $k$.}\")\n",
    "print(\"\\\\label{tab:k_sparse_comparison}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
